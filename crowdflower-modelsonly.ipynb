{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntrain.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['query'].map(lambda x:len(x.split())).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['product_title'].map(lambda x:len(x.split())).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_2.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Skipping product description as it's too lengthy and missing values\ntrain = clean_train_1\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\nparam_grid = {'C': [1], 'epsilon':[0.1,0.05], 'kernel': ('linear', 'rbf')}\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=8)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 0.26 is the best score till now\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingRegressor\n\n# Assuming you have train_x, train_y as your training data\nX_train, X_dev, y_train, y_dev = train_test_split(train_x, train_y, test_size=0.2, random_state=1)\n\n# SVR\nsvr_best_estimator = clf.best_estimator_\nsvr_best_estimator.fit(X_train, y_train)\ny_pred_svr = svr_best_estimator.predict(X_dev)\nmse_svr = mean_squared_error(y_dev, y_pred_svr)\nprint(f'Mean Squared Error (SVR): {mse_svr}')\n\n# Random Forest\nrf_best_estimator = rf.best_estimator_\nrf_best_estimator.fit(X_train, y_train)\ny_pred_rf = rf_best_estimator.predict(X_dev)\nmse_rf = mean_squared_error(y_dev, y_pred_rf)\nprint(f'Mean Squared Error (Random Forest): {mse_rf}')\n\n# TfidfVectorizer + SVM\ntfidf_svm_model = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean=False), SVC())\ntfidf_svm_model.fit(X_train, y_train)\ny_pred_svm = tfidf_svm_model.predict(X_dev)\nmse_svm = mean_squared_error(y_dev, y_pred_svm)\nprint(f'Mean Squared Error (TfidfVectorizer + SVM): {mse_svm}')\n\n# TfidfVectorizer + Naive Bayes\ntfidf_nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\ntfidf_nb_model.fit(X_train, y_train)\ny_pred_nb = tfidf_nb_model.predict(X_dev)\nmse_nb = mean_squared_error(y_dev, y_pred_nb)\nprint(f'Mean Squared Error (TfidfVectorizer + Naive Bayes): {mse_nb}')\n\n# Create the stacking regressor\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator), ('svm', tfidf_svm_model), ('nb', tfidf_nb_model)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(\n    estimators=base_models,\n    final_estimator=meta_model,\n    cv=3,  # Number of cross-validation folds\n    scoring=make_scorer(mean_squared_error, greater_is_better=False)\n)\n\n# Train the stacking regressor\nstacking_regressor.fit(X_train, y_train)\n\n# Make predictions on dev set\ny_pred_ensemble = stacking_regressor.predict(X_dev)\n\n# Evaluate the performance of the ensemble\nmse_ensemble = mean_squared_error(y_dev, y_pred_ensemble)\nprint(f'Mean Squared Error (Ensemble): {mse_ensemble}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingRegressor\n\n\nX_train, X_dev, y_train, y_dev = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n\n# Random Forest\nrf_best_estimator = rf.best_estimator_\nrf_best_estimator.fit(X_train, y_train)\ny_pred_rf = rf_best_estimator.predict(X_dev)\nmse_rf = mean_squared_error(y_dev, y_pred_rf)\nprint(f'Mean Squared Error (Random Forest): {mse_rf}')\n\n# TfidfVectorizer + SVM\ntfidf_svm_model = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean=False), SVC())\ntfidf_svm_model.fit(X_train, y_train)\ny_pred_svm = tfidf_svm_model.predict(X_dev)\nmse_svm = mean_squared_error(y_dev, y_pred_svm)\nprint(f'Mean Squared Error (TfidfVectorizer + SVM): {mse_svm}')\n\n# TfidfVectorizer + Naive Bayes\ntfidf_nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\ntfidf_nb_model.fit(X_train, y_train)\ny_pred_nb = tfidf_nb_model.predict(X_dev)\nmse_nb = mean_squared_error(y_dev, y_pred_nb)\nprint(f'Mean Squared Error (TfidfVectorizer + Naive Bayes): {mse_nb}')\n\n# Create the stacking regressor\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator), ('svm', tfidf_svm_model), ('nb', tfidf_nb_model)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(\n    estimators=base_models,\n    final_estimator=meta_model,\n    cv=3,  # Number of cross-validation folds\n    scoring=make_scorer(mean_squared_error, greater_is_better=False)\n)\n\n# Train the stacking regressor\nstacking_regressor.fit(X_train, y_train)\n\n# Make predictions on dev set\ny_pred_ensemble = stacking_regressor.predict(X_dev)\n\n# Evaluate the performance of the ensemble\nmse_ensemble = mean_squared_error(y_dev, y_pred_ensemble)\nprint(f'Mean Squared Error (Ensemble): {mse_ensemble}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = stacking_regressor.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip')\nsub\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}