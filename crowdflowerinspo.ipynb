{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T16:58:46.123568Z","iopub.execute_input":"2023-11-29T16:58:46.124629Z","iopub.status.idle":"2023-11-29T16:58:46.146687Z","shell.execute_reply.started":"2023-11-29T16:58:46.124445Z","shell.execute_reply":"2023-11-29T16:58:46.145406Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:46.148944Z","iopub.execute_input":"2023-11-29T16:58:46.149724Z","iopub.status.idle":"2023-11-29T16:58:46.607795Z","shell.execute_reply.started":"2023-11-29T16:58:46.149681Z","shell.execute_reply":"2023-11-29T16:58:46.606426Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id                      query  \\\n0   1  bridal shower decorations   \n1   2       led christmas lights   \n2   4                  projector   \n3   5                  wine rack   \n4   7                 light bulb   \n\n                                       product_title  \\\n0        Accent Pillow with Heart Design - Red/Black   \n1  Set of 10 Battery Operated Multi LED Train Chr...   \n2         ViewSonic Pro8200 DLP Multimedia Projector   \n3  Concept Housewares WR-44526 Solid-Wood Ceiling...   \n4  Wintergreen Lighting Christmas LED Light Bulb ...   \n\n                                 product_description  median_relevance  \\\n0  Red satin accent pillow embroidered with a hea...                 1   \n1  Set of 10 Battery Operated Train Christmas Lig...                 4   \n2                                                NaN                 4   \n3  Like a silent and sturdy tree, the Southern En...                 4   \n4  WTGR1011\\nFeatures\\nNickel base, 60,000 averag...                 2   \n\n   relevance_variance  \n0               0.000  \n1               0.000  \n2               0.471  \n3               0.000  \n4               0.471  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>query</th>\n      <th>product_title</th>\n      <th>product_description</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>bridal shower decorations</td>\n      <td>Accent Pillow with Heart Design - Red/Black</td>\n      <td>Red satin accent pillow embroidered with a hea...</td>\n      <td>1</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>led christmas lights</td>\n      <td>Set of 10 Battery Operated Multi LED Train Chr...</td>\n      <td>Set of 10 Battery Operated Train Christmas Lig...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>projector</td>\n      <td>ViewSonic Pro8200 DLP Multimedia Projector</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>wine rack</td>\n      <td>Concept Housewares WR-44526 Solid-Wood Ceiling...</td>\n      <td>Like a silent and sturdy tree, the Southern En...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>light bulb</td>\n      <td>Wintergreen Lighting Christmas LED Light Bulb ...</td>\n      <td>WTGR1011\\nFeatures\\nNickel base, 60,000 averag...</td>\n      <td>2</td>\n      <td>0.471</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\n\ntrain['query'] = train['query'].apply(remove_html_tags)\ntest['query'] = test['query'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:46.609313Z","iopub.execute_input":"2023-11-29T16:58:46.610143Z","iopub.status.idle":"2023-11-29T16:58:49.000424Z","shell.execute_reply.started":"2023-11-29T16:58:46.610095Z","shell.execute_reply":"2023-11-29T16:58:48.999056Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.002620Z","iopub.execute_input":"2023-11-29T16:58:49.003263Z","iopub.status.idle":"2023-11-29T16:58:49.025071Z","shell.execute_reply.started":"2023-11-29T16:58:49.003204Z","shell.execute_reply":"2023-11-29T16:58:49.023825Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10158 entries, 0 to 10157\nData columns (total 6 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   10158 non-null  int64  \n 1   query                10158 non-null  object \n 2   product_title        10158 non-null  object \n 3   product_description  7714 non-null   object \n 4   median_relevance     10158 non-null  int64  \n 5   relevance_variance   10158 non-null  float64\ndtypes: float64(1), int64(2), object(3)\nmemory usage: 476.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.029829Z","iopub.execute_input":"2023-11-29T16:58:49.030253Z","iopub.status.idle":"2023-11-29T16:58:49.056101Z","shell.execute_reply.started":"2023-11-29T16:58:49.030194Z","shell.execute_reply":"2023-11-29T16:58:49.054614Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22513 entries, 0 to 22512\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   id                   22513 non-null  int64 \n 1   query                22513 non-null  object\n 2   product_title        22513 non-null  object\n 3   product_description  17086 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 703.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.058260Z","iopub.execute_input":"2023-11-29T16:58:49.058772Z","iopub.status.idle":"2023-11-29T16:58:49.093900Z","shell.execute_reply.started":"2023-11-29T16:58:49.058722Z","shell.execute_reply":"2023-11-29T16:58:49.092537Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount  10158.000000      10158.000000        10158.000000\nmean   16353.103071          3.309805            0.377863\nstd     9447.106683          0.980666            0.389707\nmin        1.000000          1.000000            0.000000\n25%     8078.750000          3.000000            0.000000\n50%    16349.500000          4.000000            0.471000\n75%    24570.750000          4.000000            0.471000\nmax    32668.000000          4.000000            1.470000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16353.103071</td>\n      <td>3.309805</td>\n      <td>0.377863</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9447.106683</td>\n      <td>0.980666</td>\n      <td>0.389707</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8078.750000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16349.500000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>24570.750000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32668.000000</td>\n      <td>4.000000</td>\n      <td>1.470000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['query'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.096060Z","iopub.execute_input":"2023-11-29T16:58:49.096599Z","iopub.status.idle":"2023-11-29T16:58:49.121121Z","shell.execute_reply.started":"2023-11-29T16:58:49.096545Z","shell.execute_reply":"2023-11-29T16:58:49.119796Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"2    5379\n3    2819\n4     925\n1     885\n6      81\n5      69\nName: query, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['product_title'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.122822Z","iopub.execute_input":"2023-11-29T16:58:49.123202Z","iopub.status.idle":"2023-11-29T16:58:49.151524Z","shell.execute_reply.started":"2023-11-29T16:58:49.123170Z","shell.execute_reply":"2023-11-29T16:58:49.150181Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"7     1288\n6     1284\n8     1183\n9     1122\n5     1002\n10     880\n11     744\n12     678\n4      550\n13     453\n14     289\n15     181\n3      174\n17      78\n16      66\n2       51\n18      25\n19      24\n20      15\n24      11\n21       7\n25       7\n27       6\n26       6\n22       6\n28       6\n23       6\n29       4\n1        2\n46       2\n32       2\n38       1\n44       1\n34       1\n31       1\n43       1\n41       1\nName: product_title, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Feature engineering before vector","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction import text\nimport string\n\n# Initialize the Porter Stemmer\nstemmer = PorterStemmer()\n\n# Stopwords and Punctuation\nstop_words = ['http', 'www', 'img', 'border', 'color', 'style', 'padding', 'table', 'font', 'thi', 'inch', 'ha', 'width', 'height',\n              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nstop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n\npunct = string.punctuation\npunct_re = re.compile('[{}]'.format(re.escape(punct)))\n\ncolumns_to_preprocess = ['query', 'product_title', 'product_description']\n\ndef preprocess_text_column(df, column):\n    df[column] = df[column].apply(lambda x: preprocess(str(x)))\n    return df\n\ndef preprocess(x):\n    x = x.lower()\n    x = punct_re.sub(' ', x)\n    new_x = []\n    for token in x.split(' '):\n        if token not in stop_words:\n            new_x.append(stemmer.stem(token))\n    return ' '.join(new_x)\n\n# Example usage:\n# Assuming you have 'train' and 'test' DataFrames\ntrain = preprocess_text_column(train, 'query')\ntrain = preprocess_text_column(train, 'product_title')\ntrain = preprocess_text_column(train, 'product_description')\n\ntest = preprocess_text_column(test, 'query')\ntest = preprocess_text_column(test, 'product_title')\ntest = preprocess_text_column(test, 'product_description')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T16:58:49.153382Z","iopub.execute_input":"2023-11-29T16:58:49.153973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = clean_train_1\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_2.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"adding further feature engineering","metadata":{}},{"cell_type":"code","source":"print(train_input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\nsvd = TruncatedSVD(n_components=100)\ntrain_x_tfidf = svd.fit_transform(train_x_tfidf)\ndev_x_tfidf = svd.transform(dev_x_tfidf)\n\n# Add new features\ndistances = []\nquasi_jaccard = []\n\nfor row in train_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ntrain_x = np.column_stack([train_x_tfidf, np.array([distances, quasi_jaccard]).T])\n\ndistances = []\nquasi_jaccard = []\n\nfor row in dev_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ndev_x = np.column_stack([dev_x_tfidf, np.array([distances, quasi_jaccard]).T])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n# Assuming train_input and dev_input are your raw text data\nfeature_inserter = FeatureInserter()\ntrain_x_with_features = feature_inserter.fit_transform(train_input, train_y)\ndev_x_with_features = feature_inserter.transform(dev_input)\n\n# Assuming train_x and dev_x are your vectorized data\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Concatenate the new features with the TF-IDF vectorized data\ntrain = np.column_stack([train_x_tfidf.toarray(), train_x_with_features])\ndev = np.column_stack([dev_x_tfidf.toarray(), dev_x_with_features])'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.cross_validation import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# Example usage:\nfeature_inserter = FeatureInserter()\n\n# Apply TF-IDF vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Apply FeatureInserter\ntrain_x = feature_inserter.fit_transform(train_x_tfidf, train_y)\ndev_x = feature_inserter.transform(dev_x_tfidf)\n\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\nparam_grid = {'C': [1,2,5,10], 'epsilon':[0.1,0.2,0.5], 'kernel': ('linear', 'rbf', 'poly','sigmoid')}\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=-1)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Random Forest Regression\nrandom_forest_regression = RandomForestRegressor()\n\n# Parameter Grid\n'''random_forest_regression_param_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}'''\nrandom_forest_regression_param_grid = {\n    #'n_estimators': [50, 200],\n    #'max_depth': [None, 10, 30],\n    #'min_samples_split': [2, 5],\n    #'min_samples_leaf': [2, 4],\n    'bootstrap': [True, False]\n}\n\nrf = GridSearchCV(random_forest_regression, random_forest_regression_param_grid, verbose=True,scoring=scorer, n_jobs=-1, cv=1)\nrf.fit(train_x, train_y)\nrf.best_estimator_, rf.best_params_, rf.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 0.26 is the best score till now\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)\n\npreds_rf = rf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds_rf),  reg_scorer(dev_y, preds_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nsvr_best_estimator = clf.best_estimator_\nrf_best_estimator = rf.best_estimator_\n\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n\nstacking_regressor.fit(train_x, train_y)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = stacking_regressor.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = rf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip')\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}