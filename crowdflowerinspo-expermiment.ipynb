{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T17:43:44.65663Z","iopub.execute_input":"2023-11-29T17:43:44.657131Z","iopub.status.idle":"2023-11-29T17:43:44.66879Z","shell.execute_reply.started":"2023-11-29T17:43:44.657091Z","shell.execute_reply":"2023-11-29T17:43:44.667747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:44.693414Z","iopub.execute_input":"2023-11-29T17:43:44.694326Z","iopub.status.idle":"2023-11-29T17:43:45.11745Z","shell.execute_reply.started":"2023-11-29T17:43:44.694281Z","shell.execute_reply":"2023-11-29T17:43:45.116014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\n\ntrain['query'] = train['query'].apply(remove_html_tags)\ntest['query'] = test['query'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:45.120317Z","iopub.execute_input":"2023-11-29T17:43:45.121145Z","iopub.status.idle":"2023-11-29T17:43:47.384435Z","shell.execute_reply.started":"2023-11-29T17:43:45.121103Z","shell.execute_reply":"2023-11-29T17:43:47.383119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.385939Z","iopub.execute_input":"2023-11-29T17:43:47.386353Z","iopub.status.idle":"2023-11-29T17:43:47.406561Z","shell.execute_reply.started":"2023-11-29T17:43:47.386315Z","shell.execute_reply":"2023-11-29T17:43:47.405504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.408313Z","iopub.execute_input":"2023-11-29T17:43:47.40907Z","iopub.status.idle":"2023-11-29T17:43:47.432744Z","shell.execute_reply.started":"2023-11-29T17:43:47.40903Z","shell.execute_reply":"2023-11-29T17:43:47.431062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.436689Z","iopub.execute_input":"2023-11-29T17:43:47.4371Z","iopub.status.idle":"2023-11-29T17:43:47.467363Z","shell.execute_reply.started":"2023-11-29T17:43:47.437064Z","shell.execute_reply":"2023-11-29T17:43:47.46607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['query'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.469146Z","iopub.execute_input":"2023-11-29T17:43:47.470033Z","iopub.status.idle":"2023-11-29T17:43:47.492883Z","shell.execute_reply.started":"2023-11-29T17:43:47.469993Z","shell.execute_reply":"2023-11-29T17:43:47.491584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['product_title'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.49471Z","iopub.execute_input":"2023-11-29T17:43:47.495108Z","iopub.status.idle":"2023-11-29T17:43:47.522454Z","shell.execute_reply.started":"2023-11-29T17:43:47.495074Z","shell.execute_reply":"2023-11-29T17:43:47.521057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature engineering before vector","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction import text\nimport string\n\n# Initialize the Porter Stemmer\nstemmer = PorterStemmer()\n\n# Stopwords and Punctuation\nstop_words = ['http', 'www', 'img', 'border', 'color', 'style', 'padding', 'table', 'font', 'thi', 'inch', 'ha', 'width', 'height',\n              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nstop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n\npunct = string.punctuation\npunct_re = re.compile('[{}]'.format(re.escape(punct)))\n\ncolumns_to_preprocess = ['query', 'product_title', 'product_description']\n\ndef preprocess_text_column(df, column):\n    df[column] = df[column].apply(lambda x: preprocess(str(x)))\n    return df\n\ndef preprocess(x):\n    x = x.lower()\n    x = punct_re.sub(' ', x)\n    new_x = []\n    for token in x.split(' '):\n        if token not in stop_words:\n            new_x.append(stemmer.stem(token))\n    return ' '.join(new_x)\n\n# Example usage:\n# Assuming you have 'train' and 'test' DataFrames\ntrain = preprocess_text_column(train, 'query')\ntrain = preprocess_text_column(train, 'product_title')\ntrain = preprocess_text_column(train, 'product_description')\n\ntest = preprocess_text_column(test, 'query')\ntest = preprocess_text_column(test, 'product_title')\ntest = preprocess_text_column(test, 'product_description')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:43:47.523983Z","iopub.execute_input":"2023-11-29T17:43:47.524912Z","iopub.status.idle":"2023-11-29T17:45:03.170059Z","shell.execute_reply.started":"2023-11-29T17:43:47.524869Z","shell.execute_reply":"2023-11-29T17:45:03.168358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:03.171932Z","iopub.execute_input":"2023-11-29T17:45:03.172391Z","iopub.status.idle":"2023-11-29T17:45:03.179951Z","shell.execute_reply.started":"2023-11-29T17:45:03.172349Z","shell.execute_reply":"2023-11-29T17:45:03.178506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:03.181811Z","iopub.execute_input":"2023-11-29T17:45:03.182449Z","iopub.status.idle":"2023-11-29T17:45:03.217923Z","shell.execute_reply.started":"2023-11-29T17:45:03.182399Z","shell.execute_reply":"2023-11-29T17:45:03.216529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = clean_train_1\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:03.219559Z","iopub.execute_input":"2023-11-29T17:45:03.219955Z","iopub.status.idle":"2023-11-29T17:45:03.986858Z","shell.execute_reply.started":"2023-11-29T17:45:03.219919Z","shell.execute_reply":"2023-11-29T17:45:03.985648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nclass FeatureInserter(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def compute_distances(self, X):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X:\n            cos_distance = linear_kernel(row[:row.shape[0]//2], row[row.shape[0]//2:])\n            distances.append(cos_distance[0])\n            \n            intersect = row[:row.shape[0]//2].dot(row[row.shape[0]//2:])\n            union = (row[:row.shape[0]//2] + row[row.shape[0]//2:]).dot((row[:row.shape[0]//2] + row[row.shape[0]//2:]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n        return np.array(distances), np.array(quasi_jaccard)\n\n    def transform(self, X, y=None):\n        distances, quasi_jaccard = self.compute_distances(X)\n        return hstack([X, distances.reshape(-1, 1), quasi_jaccard.reshape(-1, 1)])\n\n    def fit(self, X, y=None):\n        return self\n\n    def fit_transform(self, X, y=None, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n\nfeature_inserter = FeatureInserter()\ntrain_x = feature_inserter.fit_transform(train_input)\ndev_x = feature_inserter.transform(dev_input)\ntest_x = feature_inserter.transform(test_input)\n\n# Apply TF-IDF vectorization\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_x)\ndev_x = tfidf.transform(dev_x)\ntest_x = tfidf.transform(test_x)\n\n# Now train_x and dev_x contain the TF-IDF features along with the additional distances and quasi-Jaccard features\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_x)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:03.9894Z","iopub.execute_input":"2023-11-29T17:45:03.989909Z","iopub.status.idle":"2023-11-29T17:45:04.018097Z","shell.execute_reply.started":"2023-11-29T17:45:03.98986Z","shell.execute_reply":"2023-11-29T17:45:04.017209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_2.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:04.019798Z","iopub.execute_input":"2023-11-29T17:45:04.020216Z","iopub.status.idle":"2023-11-29T17:45:04.047486Z","shell.execute_reply.started":"2023-11-29T17:45:04.020179Z","shell.execute_reply":"2023-11-29T17:45:04.046239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)\ntest_x = tfidf.transform(test_input)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:04.052718Z","iopub.execute_input":"2023-11-29T17:45:04.053471Z","iopub.status.idle":"2023-11-29T17:45:06.888195Z","shell.execute_reply.started":"2023-11-29T17:45:04.053426Z","shell.execute_reply":"2023-11-29T17:45:06.886674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_input)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:06.905119Z","iopub.execute_input":"2023-11-29T17:45:06.905536Z","iopub.status.idle":"2023-11-29T17:45:06.91701Z","shell.execute_reply.started":"2023-11-29T17:45:06.905502Z","shell.execute_reply":"2023-11-29T17:45:06.915356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:06.918753Z","iopub.execute_input":"2023-11-29T17:45:06.919215Z","iopub.status.idle":"2023-11-29T17:45:06.941631Z","shell.execute_reply.started":"2023-11-29T17:45:06.919157Z","shell.execute_reply":"2023-11-29T17:45:06.940314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\ntest_x_tfidf = tfidf.transform(test_input)\n\nsvd = TruncatedSVD(n_components=100)\ntrain_x_tfidf = svd.fit_transform(train_x_tfidf)\ndev_x_tfidf = svd.transform(dev_x_tfidf)\ntest_x_tfidf = svd.transform(test_x_tfidf)\n\n# Add new features\ndistances = []\nquasi_jaccard = []\n\nfor row in train_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ntrain_x = np.column_stack([train_x_tfidf, np.array([distances, quasi_jaccard]).T])\n\ndistances = []\nquasi_jaccard = []\n\nfor row in dev_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ndev_x = np.column_stack([dev_x_tfidf, np.array([distances, quasi_jaccard]).T])\n\ndistances = []\nquasi_jaccard = []\n\nfor row in test_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ntest_x = np.column_stack([test_x_tfidf, np.array([distances, quasi_jaccard]).T])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:06.944082Z","iopub.execute_input":"2023-11-29T17:45:06.944628Z","iopub.status.idle":"2023-11-29T17:45:24.603778Z","shell.execute_reply.started":"2023-11-29T17:45:06.944577Z","shell.execute_reply":"2023-11-29T17:45:24.602543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n# Assuming train_input and dev_input are your raw text data\nfeature_inserter = FeatureInserter()\ntrain_x_with_features = feature_inserter.fit_transform(train_input, train_y)\ndev_x_with_features = feature_inserter.transform(dev_input)\n\n# Assuming train_x and dev_x are your vectorized data\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Concatenate the new features with the TF-IDF vectorized data\ntrain = np.column_stack([train_x_tfidf.toarray(), train_x_with_features])\ndev = np.column_stack([dev_x_tfidf.toarray(), dev_x_with_features])'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:24.605755Z","iopub.execute_input":"2023-11-29T17:45:24.606128Z","iopub.status.idle":"2023-11-29T17:45:24.617862Z","shell.execute_reply.started":"2023-11-29T17:45:24.606093Z","shell.execute_reply":"2023-11-29T17:45:24.616604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.cross_validation import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# Example usage:\nfeature_inserter = FeatureInserter()\n\n# Apply TF-IDF vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Apply FeatureInserter\ntrain_x = feature_inserter.fit_transform(train_x_tfidf, train_y)\ndev_x = feature_inserter.transform(dev_x_tfidf)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:24.620202Z","iopub.execute_input":"2023-11-29T17:45:24.621623Z","iopub.status.idle":"2023-11-29T17:45:24.639044Z","shell.execute_reply.started":"2023-11-29T17:45:24.621579Z","shell.execute_reply":"2023-11-29T17:45:24.637645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:24.641143Z","iopub.execute_input":"2023-11-29T17:45:24.641584Z","iopub.status.idle":"2023-11-29T17:45:24.654972Z","shell.execute_reply.started":"2023-11-29T17:45:24.641547Z","shell.execute_reply":"2023-11-29T17:45:24.653933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:24.656164Z","iopub.execute_input":"2023-11-29T17:45:24.656602Z","iopub.status.idle":"2023-11-29T17:45:24.665186Z","shell.execute_reply.started":"2023-11-29T17:45:24.656566Z","shell.execute_reply":"2023-11-29T17:45:24.663857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\nparam_grid = {'C': [1,2,5,10], 'epsilon':[0.1,0.2,0.5], 'kernel': ('linear', 'rbf', 'poly','sigmoid')}\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=-1)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-29T17:45:24.666636Z","iopub.execute_input":"2023-11-29T17:45:24.667021Z","iopub.status.idle":"2023-11-29T18:15:56.443451Z","shell.execute_reply.started":"2023-11-29T17:45:24.666974Z","shell.execute_reply":"2023-11-29T18:15:56.442056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Random Forest Regression\nrandom_forest_regression = RandomForestRegressor()\n\n# Parameter Grid\n'''random_forest_regression_param_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}'''\nrandom_forest_regression_param_grid = {\n    #'n_estimators': [50, 200],\n    #'max_depth': [None, 10, 30],\n    #'min_samples_split': [2, 5],\n    #'min_samples_leaf': [2, 4],\n    'bootstrap': [True, False]\n}\n#RF HERE\n#rf = GridSearchCV(random_forest_regression, random_forest_regression_param_grid, verbose=True,scoring=scorer, n_jobs=-1, cv=2)\n#rf.fit(train_x, train_y)\n#rf.best_estimator_, rf.best_params_, rf.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:15:56.445152Z","iopub.execute_input":"2023-11-29T18:15:56.445578Z","iopub.status.idle":"2023-11-29T18:18:07.464208Z","shell.execute_reply.started":"2023-11-29T18:15:56.445537Z","shell.execute_reply":"2023-11-29T18:18:07.463288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 0.26 is the best score till now\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)\n\n#preds_rf = rf.best_estimator_.predict(dev_x)\n#mean_squared_error(dev_y, preds_rf),  reg_scorer(dev_y, preds_rf)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:18:07.465596Z","iopub.execute_input":"2023-11-29T18:18:07.466886Z","iopub.status.idle":"2023-11-29T18:18:08.907045Z","shell.execute_reply.started":"2023-11-29T18:18:07.466843Z","shell.execute_reply":"2023-11-29T18:18:08.906155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n'''svr_best_estimator = clf.best_estimator_\nrf_best_estimator = rf.best_estimator_\n\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n\nstacking_regressor.fit(train_x, train_y)\npreds_stacking_regressor = stacking_regressor.predict(dev_x)\nmean_squared_error(dev_y, preds_stacking_regressor),  reg_scorer(dev_y, preds_stacking_regressor)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:25:01.509656Z","iopub.execute_input":"2023-11-29T18:25:01.510166Z","iopub.status.idle":"2023-11-29T18:30:46.228655Z","shell.execute_reply.started":"2023-11-29T18:25:01.510123Z","shell.execute_reply":"2023-11-29T18:30:46.227397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\n#test_x = tfidf.transform(test_input)\n'''pred = stacking_regressor.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:32:33.424327Z","iopub.execute_input":"2023-11-29T18:32:33.424816Z","iopub.status.idle":"2023-11-29T18:32:49.052478Z","shell.execute_reply.started":"2023-11-29T18:32:33.424776Z","shell.execute_reply":"2023-11-29T18:32:49.050824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\n#test_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:23:50.727596Z","iopub.status.idle":"2023-11-29T18:23:50.729056Z","shell.execute_reply.started":"2023-11-29T18:23:50.72881Z","shell.execute_reply":"2023-11-29T18:23:50.728839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = rf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:23:50.730616Z","iopub.status.idle":"2023-11-29T18:23:50.73113Z","shell.execute_reply.started":"2023-11-29T18:23:50.730884Z","shell.execute_reply":"2023-11-29T18:23:50.730906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip')\nsub","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:23:50.733507Z","iopub.status.idle":"2023-11-29T18:23:50.734172Z","shell.execute_reply.started":"2023-11-29T18:23:50.733936Z","shell.execute_reply":"2023-11-29T18:23:50.733958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out","metadata":{"execution":{"iopub.status.busy":"2023-11-29T18:23:50.73576Z","iopub.status.idle":"2023-11-29T18:23:50.736261Z","shell.execute_reply.started":"2023-11-29T18:23:50.736015Z","shell.execute_reply":"2023-11-29T18:23:50.736036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}