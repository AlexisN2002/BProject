{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T17:11:49.475969Z","iopub.execute_input":"2023-11-16T17:11:49.476711Z","iopub.status.idle":"2023-11-16T17:11:49.490995Z","shell.execute_reply.started":"2023-11-16T17:11:49.476657Z","shell.execute_reply":"2023-11-16T17:11:49.489010Z"},"trusted":true},"execution_count":244,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntest['relevance_variance'] = np.NaN\ntraining['train_test'] = 1\ntraining = training.drop(['product_description'], axis=1)\ntest['train_test'] = 0\nall_data = pd.concat([training,test]) \nprint(training.head())\nprint(test.head())\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \nX_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)\nX_check = X_train #for heatmap\nX_train = X_train.drop(['relevance_variance'], axis=1)\nX_backup = X_train\nX_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\ny_train = all_data[all_data.train_test==1].relevance_variance\ny_backup = y_train\nX_test = X_test.drop(['relevance_variance'], axis=1)\nfor column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:11:49.496622Z","iopub.execute_input":"2023-11-16T17:11:49.497102Z","iopub.status.idle":"2023-11-16T17:12:00.850819Z","shell.execute_reply.started":"2023-11-16T17:11:49.497057Z","shell.execute_reply":"2023-11-16T17:12:00.848377Z"},"trusted":true},"execution_count":245,"outputs":[{"name":"stderr","text":"wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"   id                      query  \\\n0   1  bridal shower decorations   \n1   2       led christmas lights   \n2   4                  projector   \n3   5                  wine rack   \n4   7                 light bulb   \n\n                                       product_title  median_relevance  \\\n0        Accent Pillow with Heart Design - Red/Black                 1   \n1  Set of 10 Battery Operated Multi LED Train Chr...                 4   \n2         ViewSonic Pro8200 DLP Multimedia Projector                 4   \n3  Concept Housewares WR-44526 Solid-Wood Ceiling...                 4   \n4  Wintergreen Lighting Christmas LED Light Bulb ...                 2   \n\n   relevance_variance  train_test  \n0               0.000           1  \n1               0.000           1  \n2               0.471           1  \n3               0.000           1  \n4               0.471           1  \n   id                           query  \\\n0   3                electric griddle   \n1   6           phillips coffee maker   \n2   9             san francisco 49ers   \n3  11                  aveeno shampoo   \n4  12  flea and tick control for dogs   \n\n                                       product_title  \\\n0                    Star-Max 48 in Electric Griddle   \n1  Philips SENSEO HD7810 WHITE Single Serve Pod C...   \n2                     2013 San Francisco 49ers Clock   \n3                AVEENO       10.5FLOZ NRSH SHINE SH   \n4  Merial Frontline Plus Flea and Tick Control fo...   \n\n                                 product_description  relevance_variance  \\\n0                                                NaN                 NaN   \n1                                                NaN                 NaN   \n2  A 2013 San Francisco 49ers clock is the ultima...                 NaN   \n3  Water, Ammonium Lauryl Sulfate, Dimethicone, S...                 NaN   \n4                                                NaN                 NaN   \n\n   train_test  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[245], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m attribute_names \u001b[38;5;241m=\u001b[39m all_data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     17\u001b[0m test_atts \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 18\u001b[0m all_dummies \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattribute_names\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m test_dummies \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(test[test_atts])  \n\u001b[1;32m     20\u001b[0m X_train \u001b[38;5;241m=\u001b[39m all_dummies[all_data\u001b[38;5;241m.\u001b[39mtrain_test \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_test\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/encoding.py:203\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    199\u001b[0m     with_dummies \u001b[38;5;241m=\u001b[39m [data\u001b[38;5;241m.\u001b[39mselect_dtypes(exclude\u001b[38;5;241m=\u001b[39mdtypes_to_encode)]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col, pre, sep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(data_to_encode\u001b[38;5;241m.\u001b[39mitems(), prefix, prefix_sep):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# col is (column_name, column), use just column data here\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     dummy \u001b[38;5;241m=\u001b[39m \u001b[43m_get_dummies_1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefix_sep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdummy_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdummy_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     with_dummies\u001b[38;5;241m.\u001b[39mappend(dummy)\n\u001b[1;32m    213\u001b[0m result \u001b[38;5;241m=\u001b[39m concat(with_dummies, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/reshape/encoding.py:324\u001b[0m, in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     eye_dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_\n\u001b[0;32m--> 324\u001b[0m dummy_mat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber_of_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meye_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dummy_na:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# reset NaN GH4446\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     dummy_mat[codes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print(training)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.852189Z","iopub.status.idle":"2023-11-16T17:12:00.853087Z","shell.execute_reply.started":"2023-11-16T17:12:00.852821Z","shell.execute_reply":"2023-11-16T17:12:00.852845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.854428Z","iopub.status.idle":"2023-11-16T17:12:00.854853Z","shell.execute_reply.started":"2023-11-16T17:12:00.854648Z","shell.execute_reply":"2023-11-16T17:12:00.854668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntarget_variable = 'relevance_variance'\nnumeric_data = training.select_dtypes(include='number')\ncorrelation_matrix = numeric_data.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix[[target_variable]], annot=True, cmap='coolwarm', linewidths=.5)\n\nplt.title(f'Correlation Heatmap with {target_variable}')\nplt.show()\n\n#due to the lack of numerical values, these types of visualization provide little information","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.856496Z","iopub.status.idle":"2023-11-16T17:12:00.857623Z","shell.execute_reply.started":"2023-11-16T17:12:00.857373Z","shell.execute_reply":"2023-11-16T17:12:00.857398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''target_variable = 'relevance_variance'\ncorrelations = X_train.corr()[target_column]\ncorrelations = correlations.drop(target_variable)\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.1) | (correlations <= -0.1)]\nsignificant_features_index = correlations[(correlations >= 0.1) | (correlations <= -0.1)].index\nprint(significant_features)\nprint(significant_features_index)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()'''","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.858936Z","iopub.status.idle":"2023-11-16T17:12:00.859372Z","shell.execute_reply.started":"2023-11-16T17:12:00.859172Z","shell.execute_reply":"2023-11-16T17:12:00.859193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train = pd.concat([X_train, y_train], axis=1)\n\ndef create_splits(X_train, y_train, num_splits):\n    total_features = X_train.shape[1]\n    features_per_subset = total_features // num_splits\n    all_train_splits = []\n    for i in range(num_splits):\n        start_idx = i * features_per_subset\n        end_idx = (i + 1) * features_per_subset if i < num_splits - 1 else total_features\n        subset_features = X_train.iloc[:, start_idx:end_idx]\n        all_train_subset = pd.concat([subset_features, y_train], axis=1)\n        all_train_splits.append(all_train_subset)\n\n    return all_train_splits\n\n# Example usage:\nnum_splits = 2000\nall_train_splits = create_splits(X_train, y_train, num_splits)\n\n\n#print(all_train_splits[0].head())\nplt.figure(figsize=(16, 8))\ncorrelations = {}\n\n'''def feature_stats(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])\n    target = dataset[targ]\n\n    # Calculate feature correlation with the target\n    feature_correlation = features.corrwith(target)\n\n    # Filter features based on correlation threshold\n    selected_features = feature_correlation[abs(feature_correlation) >= correlation_threshold]\n\n    # Create a DataFrame to store the results\n    stats_df = pd.DataFrame({'Correlation': selected_features})\n    print('###############')\n    print(selected_features)\n    print('###############')\n\n    return stats_df\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint(stats_all_train_splits_0)'''\n\nfrom scipy.stats import pearsonr\n\ndef feature_stats(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])\n    target = dataset[targ]\n\n    # Calculate feature correlation with the target and p-values\n    correlation_results = [pearsonr(features[feature], target) for feature in features.columns]\n\n    # Separate positive and negative correlations based on the threshold\n    selected_positive = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr >= correlation_threshold]\n    selected_negative = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr <= -correlation_threshold]\n\n    # Create a DataFrame to store the results\n    stats_df = pd.DataFrame({\n        'Feature': [item[0] for item in selected_positive + selected_negative],\n        'Correlation': [item[1] for item in selected_positive + selected_negative],\n        'P-Value': [item[2] for item in selected_positive + selected_negative],\n    })\n\n    return stats_df\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint('###############')\nprint(stats_all_train_splits_0)\nprint('###############')\n\n\n'''def plot_feature_heatmap(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])  \n    target = dataset[targ]\n    feature_correlation = features.corrwith(target)\n    selected_features = feature_correlation[(feature_correlation >= correlation_threshold) | (feature_correlation <= -correlation_threshold)]\n    corr_df = pd.DataFrame({'Correlation': selected_features})\n    if not corr_df.empty:\n        plt.figure(figsize=(40, 10))\n        sns.heatmap(corr_df.transpose(), annot=True, cmap='coolwarm', linewidths=.5)\n        plt.title('Feature Correlation Heatmap')\n        plt.show()\n    else:\n        print(\"No features meet the correlation threshold criteria.\")'''\ndef plot_feature_heatmap(dataset, targ, correlation_threshold=0.1):\n    # Extract features and target\n    features = dataset.drop(columns=[targ])  \n    target = dataset[targ]\n    \n    # Calculate feature correlation with the target\n    feature_correlation = features.corrwith(target)\n\n    # Filter features based on correlation threshold\n    selected_features = feature_correlation[(feature_correlation >= correlation_threshold) | (feature_correlation <= -correlation_threshold)]\n\n    # Create a DataFrame to store the results\n    corr_df = pd.DataFrame({'Correlation': selected_features})\n\n    # Print every selected feature with its correlation\n    feature_correlation_list = list(zip(selected_features.index, selected_features.values))\n    print(\"Selected Feature Correlations:\")\n    for feature, correlation in feature_correlation_list:\n        print(f\"{feature}: {correlation}\")\n\n    # Create a heatmap if there are selected features\n    if not corr_df.empty:\n        plt.figure(figsize=(50, 20))\n        sns.heatmap(corr_df.transpose(), annot=True, cmap='coolwarm', linewidths=.5)\n        plt.title('Feature Correlation Heatmap')\n        plt.show()\n    else:\n        print(\"No features meet the correlation threshold criteria.\")\n\n    return feature_correlation_list\n\nplot_feature_heatmap(all_train_splits[0], 'relevance_variance')\n\n'''for column in X_train.columns:\n    if X_train[column].nunique() == 2:  \n        plt.subplot(1, 2, 1)\n        sns.countplot(x=column, data=all_train)\n        plt.title(f'Prevalence of {column} being 1')\n\n        plt.subplot(1, 2, 2)\n        correlation = full_dataset[column].corr(y_train)\n        sns.barplot(x=[column], y=[correlation])\n        correlations[column] = correlation\n\nplt.show()\n\n# Display correlation values\nprint(\"Correlation with the target variable:\")\nfor feature, correlation in correlations.items():\n    print(f\"{feature}: {correlation:.2f}\")'''","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.860844Z","iopub.status.idle":"2023-11-16T17:12:00.861301Z","shell.execute_reply.started":"2023-11-16T17:12:00.861107Z","shell.execute_reply":"2023-11-16T17:12:00.861128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(20):\n    plot_feature_heatmap(all_train_splits[i], 'relevance_variance')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.863161Z","iopub.status.idle":"2023-11-16T17:12:00.863575Z","shell.execute_reply.started":"2023-11-16T17:12:00.863378Z","shell.execute_reply":"2023-11-16T17:12:00.863397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''plt.figure(figsize=(16, 8))\ncorrelations = {}\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint(stats_all_train_splits_0)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.866037Z","iopub.status.idle":"2023-11-16T17:12:00.866659Z","shell.execute_reply.started":"2023-11-16T17:12:00.866411Z","shell.execute_reply":"2023-11-16T17:12:00.866443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''all_train = pd.concat([X_train, y_train], axis=1)\n\ndef plot_significance(dataset, target_column, subplot_title):\n    # Create a bar plot for each binary feature\n    correlations = {}\n\n    for column in dataset.columns:\n        if dataset[column].nunique() == 2:\n            plt.subplot(1, 2, 1)\n            sns.countplot(x=column, data=dataset)\n            plt.title(f'Prevalence of {column} being 1')\n\n            plt.subplot(1, 2, 2)\n            correlation = dataset[column].corr(target_column)\n            sns.barplot(x=[column], y=[correlation])\n            correlations[column] = correlation\n\n    plt.suptitle(subplot_title)\n    plt.show()\n\nnum_splits = 5 \n\nall_train_splits = np.array_split(all_train, num_splits)\nfor i, part in enumerate(all_train_splits):\n    globals()[f'all_train_{i + 1}'] = part\n\nfor idx in range(num_splits):\n    # all_train_1 etc\n    current_split = globals()[f'all_train_{idx + 1}']\n    X_part = X_train\n    y_part = y_train\n\n    # Call the plotting function for each split\n    #plot_significance(current_split, y_part, f'Split {idx + 1}')\n\n# If you want to plot a specific split individually, you can do so like this:\n# plot_significance(all_train_1, all_train_1['your_target_column_name'], 'Split 1')'''","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.868528Z","iopub.status.idle":"2023-11-16T17:12:00.869665Z","shell.execute_reply.started":"2023-11-16T17:12:00.869447Z","shell.execute_reply":"2023-11-16T17:12:00.869471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_significance(all_train_1, all_train_1['relevance_variance'], 'Split 1')","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.870866Z","iopub.status.idle":"2023-11-16T17:12:00.872010Z","shell.execute_reply.started":"2023-11-16T17:12:00.871762Z","shell.execute_reply":"2023-11-16T17:12:00.871785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#attribute_names = X_train.columns.tolist()\n#print(attribute_names)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:12:00.873128Z","iopub.status.idle":"2023-11-16T17:12:00.873898Z","shell.execute_reply.started":"2023-11-16T17:12:00.873646Z","shell.execute_reply":"2023-11-16T17:12:00.873676Z"},"trusted":true},"execution_count":null,"outputs":[]}]}