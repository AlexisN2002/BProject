{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T21:55:26.883305Z","iopub.execute_input":"2023-11-29T21:55:26.883659Z","iopub.status.idle":"2023-11-29T21:55:26.896549Z","shell.execute_reply.started":"2023-11-29T21:55:26.883621Z","shell.execute_reply":"2023-11-29T21:55:26.895214Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:26.898595Z","iopub.execute_input":"2023-11-29T21:55:26.899656Z","iopub.status.idle":"2023-11-29T21:55:27.369325Z","shell.execute_reply.started":"2023-11-29T21:55:26.899599Z","shell.execute_reply":"2023-11-29T21:55:27.368384Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"   id                      query  \\\n0   1  bridal shower decorations   \n1   2       led christmas lights   \n2   4                  projector   \n3   5                  wine rack   \n4   7                 light bulb   \n\n                                       product_title  \\\n0        Accent Pillow with Heart Design - Red/Black   \n1  Set of 10 Battery Operated Multi LED Train Chr...   \n2         ViewSonic Pro8200 DLP Multimedia Projector   \n3  Concept Housewares WR-44526 Solid-Wood Ceiling...   \n4  Wintergreen Lighting Christmas LED Light Bulb ...   \n\n                                 product_description  median_relevance  \\\n0  Red satin accent pillow embroidered with a hea...                 1   \n1  Set of 10 Battery Operated Train Christmas Lig...                 4   \n2                                                NaN                 4   \n3  Like a silent and sturdy tree, the Southern En...                 4   \n4  WTGR1011\\nFeatures\\nNickel base, 60,000 averag...                 2   \n\n   relevance_variance  \n0               0.000  \n1               0.000  \n2               0.471  \n3               0.000  \n4               0.471  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>query</th>\n      <th>product_title</th>\n      <th>product_description</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>bridal shower decorations</td>\n      <td>Accent Pillow with Heart Design - Red/Black</td>\n      <td>Red satin accent pillow embroidered with a hea...</td>\n      <td>1</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>led christmas lights</td>\n      <td>Set of 10 Battery Operated Multi LED Train Chr...</td>\n      <td>Set of 10 Battery Operated Train Christmas Lig...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>projector</td>\n      <td>ViewSonic Pro8200 DLP Multimedia Projector</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>wine rack</td>\n      <td>Concept Housewares WR-44526 Solid-Wood Ceiling...</td>\n      <td>Like a silent and sturdy tree, the Southern En...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>light bulb</td>\n      <td>Wintergreen Lighting Christmas LED Light Bulb ...</td>\n      <td>WTGR1011\\nFeatures\\nNickel base, 60,000 averag...</td>\n      <td>2</td>\n      <td>0.471</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.370817Z","iopub.execute_input":"2023-11-29T21:55:27.371808Z","iopub.status.idle":"2023-11-29T21:55:27.392345Z","shell.execute_reply.started":"2023-11-29T21:55:27.371756Z","shell.execute_reply":"2023-11-29T21:55:27.391255Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10158 entries, 0 to 10157\nData columns (total 6 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   10158 non-null  int64  \n 1   query                10158 non-null  object \n 2   product_title        10158 non-null  object \n 3   product_description  7714 non-null   object \n 4   median_relevance     10158 non-null  int64  \n 5   relevance_variance   10158 non-null  float64\ndtypes: float64(1), int64(2), object(3)\nmemory usage: 476.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.395490Z","iopub.execute_input":"2023-11-29T21:55:27.396130Z","iopub.status.idle":"2023-11-29T21:55:27.420457Z","shell.execute_reply.started":"2023-11-29T21:55:27.396023Z","shell.execute_reply":"2023-11-29T21:55:27.419055Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22513 entries, 0 to 22512\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   id                   22513 non-null  int64 \n 1   query                22513 non-null  object\n 2   product_title        22513 non-null  object\n 3   product_description  17086 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 703.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.421771Z","iopub.execute_input":"2023-11-29T21:55:27.422115Z","iopub.status.idle":"2023-11-29T21:55:27.448741Z","shell.execute_reply.started":"2023-11-29T21:55:27.422075Z","shell.execute_reply":"2023-11-29T21:55:27.447922Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount  10158.000000      10158.000000        10158.000000\nmean   16353.103071          3.309805            0.377863\nstd     9447.106683          0.980666            0.389707\nmin        1.000000          1.000000            0.000000\n25%     8078.750000          3.000000            0.000000\n50%    16349.500000          4.000000            0.471000\n75%    24570.750000          4.000000            0.471000\nmax    32668.000000          4.000000            1.470000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16353.103071</td>\n      <td>3.309805</td>\n      <td>0.377863</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9447.106683</td>\n      <td>0.980666</td>\n      <td>0.389707</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8078.750000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16349.500000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>24570.750000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32668.000000</td>\n      <td>4.000000</td>\n      <td>1.470000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['query'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.449731Z","iopub.execute_input":"2023-11-29T21:55:27.449991Z","iopub.status.idle":"2023-11-29T21:55:27.470857Z","shell.execute_reply.started":"2023-11-29T21:55:27.449959Z","shell.execute_reply":"2023-11-29T21:55:27.469717Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"2    5379\n3    2819\n4     925\n1     885\n6      81\n5      69\nName: query, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['product_title'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.472355Z","iopub.execute_input":"2023-11-29T21:55:27.472645Z","iopub.status.idle":"2023-11-29T21:55:27.499970Z","shell.execute_reply.started":"2023-11-29T21:55:27.472600Z","shell.execute_reply":"2023-11-29T21:55:27.498908Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"7     1288\n6     1284\n8     1183\n9     1122\n5     1002\n10     880\n11     744\n12     678\n4      550\n13     453\n14     289\n15     181\n3      174\n17      78\n16      66\n2       51\n18      25\n19      24\n20      15\n24      11\n21       7\n25       7\n27       6\n26       6\n22       6\n28       6\n23       6\n29       4\n1        2\n46       2\n32       2\n38       1\n44       1\n34       1\n31       1\n43       1\n41       1\nName: product_title, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.501690Z","iopub.execute_input":"2023-11-29T21:55:27.501979Z","iopub.status.idle":"2023-11-29T21:55:27.510128Z","shell.execute_reply.started":"2023-11-29T21:55:27.501941Z","shell.execute_reply":"2023-11-29T21:55:27.508975Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.512766Z","iopub.execute_input":"2023-11-29T21:55:27.513179Z","iopub.status.idle":"2023-11-29T21:55:27.545942Z","shell.execute_reply.started":"2023-11-29T21:55:27.513135Z","shell.execute_reply":"2023-11-29T21:55:27.545056Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   2032.000000       2032.000000         2032.000000\nmean   29406.014764          3.319390            0.361364\nstd     1870.217123          0.972218            0.379619\nmin    26215.000000          1.000000            0.000000\n25%    27777.250000          3.000000            0.000000\n50%    29410.000000          4.000000            0.471000\n75%    31014.750000          4.000000            0.471000\nmax    32668.000000          4.000000            1.470000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2032.000000</td>\n      <td>2032.000000</td>\n      <td>2032.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29406.014764</td>\n      <td>3.319390</td>\n      <td>0.361364</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1870.217123</td>\n      <td>0.972218</td>\n      <td>0.379619</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>26215.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>27777.250000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>29410.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>31014.750000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32668.000000</td>\n      <td>4.000000</td>\n      <td>1.470000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"clean_train_1.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.550039Z","iopub.execute_input":"2023-11-29T21:55:27.550455Z","iopub.status.idle":"2023-11-29T21:55:27.574738Z","shell.execute_reply.started":"2023-11-29T21:55:27.550418Z","shell.execute_reply":"2023-11-29T21:55:27.573749Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   7558.000000       7558.000000         7558.000000\nmean   13074.201111          3.344403            0.321038\nstd     7571.543134          0.974908            0.332482\nmin        1.000000          1.000000            0.000000\n25%     6496.500000          3.000000            0.000000\n50%    13129.000000          4.000000            0.471000\n75%    19563.250000          4.000000            0.471000\nmax    26208.000000          4.000000            0.980000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7558.000000</td>\n      <td>7558.000000</td>\n      <td>7558.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13074.201111</td>\n      <td>3.344403</td>\n      <td>0.321038</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7571.543134</td>\n      <td>0.974908</td>\n      <td>0.332482</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6496.500000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13129.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>19563.250000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>26208.000000</td>\n      <td>4.000000</td>\n      <td>0.980000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"clean_train_2.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.576264Z","iopub.execute_input":"2023-11-29T21:55:27.576533Z","iopub.status.idle":"2023-11-29T21:55:27.601080Z","shell.execute_reply.started":"2023-11-29T21:55:27.576498Z","shell.execute_reply":"2023-11-29T21:55:27.600056Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   6206.000000       6206.000000         6206.000000\nmean   13154.066549          3.432646            0.202590\nstd     7570.559783          0.959901            0.232434\nmin        1.000000          1.000000            0.000000\n25%     6576.750000          3.000000            0.000000\n50%    13299.500000          4.000000            0.000000\n75%    19708.000000          4.000000            0.471000\nmax    26208.000000          4.000000            0.490000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6206.000000</td>\n      <td>6206.000000</td>\n      <td>6206.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13154.066549</td>\n      <td>3.432646</td>\n      <td>0.202590</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7570.559783</td>\n      <td>0.959901</td>\n      <td>0.232434</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6576.750000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13299.500000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>19708.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>26208.000000</td>\n      <td>4.000000</td>\n      <td>0.490000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Skipping product description as it's too lengthy and missing values\ntrain = clean_train_1\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.602937Z","iopub.execute_input":"2023-11-29T21:55:27.603193Z","iopub.status.idle":"2023-11-29T21:55:27.848657Z","shell.execute_reply.started":"2023-11-29T21:55:27.603160Z","shell.execute_reply":"2023-11-29T21:55:27.847257Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:27.850371Z","iopub.execute_input":"2023-11-29T21:55:27.850746Z","iopub.status.idle":"2023-11-29T21:55:29.328244Z","shell.execute_reply.started":"2023-11-29T21:55:27.850691Z","shell.execute_reply":"2023-11-29T21:55:29.327081Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:29.330347Z","iopub.execute_input":"2023-11-29T21:55:29.330736Z","iopub.status.idle":"2023-11-29T21:55:29.348515Z","shell.execute_reply.started":"2023-11-29T21:55:29.330683Z","shell.execute_reply":"2023-11-29T21:55:29.347390Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"(0.781467760430449, 1.0, 0.0)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:29.350541Z","iopub.execute_input":"2023-11-29T21:55:29.350934Z","iopub.status.idle":"2023-11-29T21:55:29.361008Z","shell.execute_reply.started":"2023-11-29T21:55:29.350883Z","shell.execute_reply":"2023-11-29T21:55:29.359702Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\n'''param_grid = {'C': [1], 'epsilon':[0.1,0.05], 'kernel': ('linear', 'rbf')}\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=8)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_'''","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-11-29T21:55:29.363156Z","iopub.execute_input":"2023-11-29T21:55:29.363527Z","iopub.status.idle":"2023-11-29T21:55:29.376592Z","shell.execute_reply.started":"2023-11-29T21:55:29.363479Z","shell.execute_reply":"2023-11-29T21:55:29.375924Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"\"param_grid = {'C': [1], 'epsilon':[0.1,0.05], 'kernel': ('linear', 'rbf')}\\nsvr  = SVR()\\nscorer = make_scorer(reg_scorer, greater_is_better=True)\\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=8)\\nclf.fit(train_x, train_y)\\nclf.best_estimator_, clf.best_params_, clf.best_score_\""},"metadata":{}}]},{"cell_type":"code","source":"## 0.26 is the best score till now\n\n#preds = clf.best_estimator_.predict(dev_x)\n#mean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:29.377745Z","iopub.execute_input":"2023-11-29T21:55:29.378531Z","iopub.status.idle":"2023-11-29T21:55:29.390276Z","shell.execute_reply.started":"2023-11-29T21:55:29.378486Z","shell.execute_reply":"2023-11-29T21:55:29.389248Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingRegressor\n\n# Assuming you have train_x, train_y as your training data\nX_train, X_dev, y_train, y_dev = train_test_split(train_x, train_y, test_size=0.2, random_state=1)\n\n# Define SVR and perform GridSearchCV\nparam_grid_svr = {'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 0.2], 'kernel': ['linear', 'rbf']}\nsvr = SVR()\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\nclf = GridSearchCV(svr, param_grid_svr, verbose=True, scoring=scorer, n_jobs=-1)\nclf.fit(X_train, y_train)\nsvr_best_estimator = clf.best_estimator_\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)\n\n# Define Random Forest and perform GridSearchCV\nparam_grid_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\nrandom_forest_regression = RandomForestRegressor()\nrf = GridSearchCV(random_forest_regression, param_grid_rf, verbose=True, scoring=scorer, n_jobs=-1, cv=3)\nrf.fit(X_train, y_train)\nrf_best_estimator = rf.best_estimator_\n\npreds = rf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)\n\n# TfidfVectorizer + SVM\ntfidf_svm_model = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean=False), SVC())\n\n# TfidfVectorizer + Naive Bayes\ntfidf_nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n\n# SVR\ny_pred_svr = svr_best_estimator.predict(X_dev)\nmse_svr = mean_squared_error(y_dev, y_pred_svr)\nprint(f'Mean Squared Error (SVR): {mse_svr}')\n\n# Random Forest\ny_pred_rf = rf_best_estimator.predict(X_dev)\nmse_rf = mean_squared_error(y_dev, y_pred_rf)\nprint(f'Mean Squared Error (Random Forest): {mse_rf}')\n\n# TfidfVectorizer + SVM\ny_pred_svm = tfidf_svm_model.predict(X_dev)\nmse_svm = mean_squared_error(y_dev, y_pred_svm)\nprint(f'Mean Squared Error (TfidfVectorizer + SVM): {mse_svm}')\n\n# TfidfVectorizer + Naive Bayes\ny_pred_nb = tfidf_nb_model.predict(X_dev)\nmse_nb = mean_squared_error(y_dev, y_pred_nb)\nprint(f'Mean Squared Error (TfidfVectorizer + Naive Bayes): {mse_nb}')\n\n# Create the stacking regressor\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator), ('svm', tfidf_svm_model), ('nb', tfidf_nb_model)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(\n    estimators=base_models,\n    final_estimator=meta_model,\n    cv=3,  # Number of cross-validation folds\n    scoring=make_scorer(mean_squared_error, greater_is_better=False)\n)\n\n# Train the stacking regressor\nstacking_regressor.fit(X_train, y_train)\n\n# Make predictions on dev set\ny_pred_ensemble = stacking_regressor.predict(X_dev)\n\n# Evaluate the performance of the ensemble\nmse_ensemble = mean_squared_error(y_dev, y_pred_ensemble)\nprint(f'Mean Squared Error (Ensemble): {mse_ensemble}')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:55:29.392384Z","iopub.execute_input":"2023-11-29T21:55:29.392805Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 18 candidates, totalling 90 fits\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n","output_type":"stream"}]},{"cell_type":"code","source":"'''from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import StackingRegressor\n\n\nX_train, X_dev, y_train, y_dev = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n\n# Random Forest\nrf_best_estimator = rf.best_estimator_\nrf_best_estimator.fit(X_train, y_train)\ny_pred_rf = rf_best_estimator.predict(X_dev)\nmse_rf = mean_squared_error(y_dev, y_pred_rf)\nprint(f'Mean Squared Error (Random Forest): {mse_rf}')\n\n# TfidfVectorizer + SVM\ntfidf_svm_model = make_pipeline(TfidfVectorizer(), StandardScaler(with_mean=False), SVC())\ntfidf_svm_model.fit(X_train, y_train)\ny_pred_svm = tfidf_svm_model.predict(X_dev)\nmse_svm = mean_squared_error(y_dev, y_pred_svm)\nprint(f'Mean Squared Error (TfidfVectorizer + SVM): {mse_svm}')\n\n# TfidfVectorizer + Naive Bayes\ntfidf_nb_model = make_pipeline(TfidfVectorizer(), MultinomialNB())\ntfidf_nb_model.fit(X_train, y_train)\ny_pred_nb = tfidf_nb_model.predict(X_dev)\nmse_nb = mean_squared_error(y_dev, y_pred_nb)\nprint(f'Mean Squared Error (TfidfVectorizer + Naive Bayes): {mse_nb}')\n\n# Create the stacking regressor\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator), ('svm', tfidf_svm_model), ('nb', tfidf_nb_model)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(\n    estimators=base_models,\n    final_estimator=meta_model,\n    cv=3,  # Number of cross-validation folds\n    scoring=make_scorer(mean_squared_error, greater_is_better=False)\n)\n\n# Train the stacking regressor\nstacking_regressor.fit(X_train, y_train)\n\n# Make predictions on dev set\ny_pred_ensemble = stacking_regressor.predict(X_dev)\n\n# Evaluate the performance of the ensemble\nmse_ensemble = mean_squared_error(y_dev, y_pred_ensemble)\nprint(f'Mean Squared Error (Ensemble): {mse_ensemble}')'''\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = stacking_regressor.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip')\nsub\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}