{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-16T17:05:24.462828Z","iopub.execute_input":"2023-11-16T17:05:24.464243Z","iopub.status.idle":"2023-11-16T17:05:24.477695Z","shell.execute_reply.started":"2023-11-16T17:05:24.464170Z","shell.execute_reply":"2023-11-16T17:05:24.475472Z"},"trusted":true},"execution_count":218,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntest['relevance_variance'] = np.NaN\ntraining['train_test'] = 1\ntraining = training.drop(['product_description'], axis=1)\ntest['train_test'] = 0\nall_data = pd.concat([training,test]) \nprint(training.head())\nprint(test.head())\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \nX_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)\nX_check = X_train #for heatmap\nX_train = X_train.drop(['relevance_variance'], axis=1)\nX_backup = X_train\nX_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\ny_train = all_data[all_data.train_test==1].relevance_variance\ny_backup = y_train\nX_test = X_test.drop(['relevance_variance'], axis=1)\nfor column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-16T17:05:24.480505Z","iopub.execute_input":"2023-11-16T17:05:24.481204Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"   id                      query  \\\n0   1  bridal shower decorations   \n1   2       led christmas lights   \n2   4                  projector   \n3   5                  wine rack   \n4   7                 light bulb   \n\n                                       product_title  median_relevance  \\\n0        Accent Pillow with Heart Design - Red/Black                 1   \n1  Set of 10 Battery Operated Multi LED Train Chr...                 4   \n2         ViewSonic Pro8200 DLP Multimedia Projector                 4   \n3  Concept Housewares WR-44526 Solid-Wood Ceiling...                 4   \n4  Wintergreen Lighting Christmas LED Light Bulb ...                 2   \n\n   relevance_variance  train_test  \n0               0.000           1  \n1               0.000           1  \n2               0.471           1  \n3               0.000           1  \n4               0.471           1  \n   id                           query  \\\n0   3                electric griddle   \n1   6           phillips coffee maker   \n2   9             san francisco 49ers   \n3  11                  aveeno shampoo   \n4  12  flea and tick control for dogs   \n\n                                       product_title  \\\n0                    Star-Max 48 in Electric Griddle   \n1  Philips SENSEO HD7810 WHITE Single Serve Pod C...   \n2                     2013 San Francisco 49ers Clock   \n3                AVEENO       10.5FLOZ NRSH SHINE SH   \n4  Merial Frontline Plus Flea and Tick Control fo...   \n\n                                 product_description  relevance_variance  \\\n0                                                NaN                 NaN   \n1                                                NaN                 NaN   \n2  A 2013 San Francisco 49ers clock is the ultima...                 NaN   \n3  Water, Ammonium Lauryl Sulfate, Dimethicone, S...                 NaN   \n4                                                NaN                 NaN   \n\n   train_test  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n","output_type":"stream"}]},{"cell_type":"code","source":"print(training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntarget_variable = 'relevance_variance'\nnumeric_data = training.select_dtypes(include='number')\ncorrelation_matrix = numeric_data.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix[[target_variable]], annot=True, cmap='coolwarm', linewidths=.5)\n\nplt.title(f'Correlation Heatmap with {target_variable}')\nplt.show()\n\n#due to the lack of numerical values, these types of visualization provide little information","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''target_variable = 'relevance_variance'\ncorrelations = X_train.corr()[target_column]\ncorrelations = correlations.drop(target_variable)\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.1) | (correlations <= -0.1)]\nsignificant_features_index = correlations[(correlations >= 0.1) | (correlations <= -0.1)].index\nprint(significant_features)\nprint(significant_features_index)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train = pd.concat([X_train, y_train], axis=1)\n\ndef create_splits(X_train, y_train, num_splits):\n    total_features = X_train.shape[1]\n    features_per_subset = total_features // num_splits\n    all_train_splits = []\n    for i in range(num_splits):\n        start_idx = i * features_per_subset\n        end_idx = (i + 1) * features_per_subset if i < num_splits - 1 else total_features\n        subset_features = X_train.iloc[:, start_idx:end_idx]\n        all_train_subset = pd.concat([subset_features, y_train], axis=1)\n        all_train_splits.append(all_train_subset)\n\n    return all_train_splits\n\n# Example usage:\nnum_splits = 1000\nall_train_splits = create_splits(X_train, y_train, num_splits)\n\n'''for i in range(split):\n    start_idx = i * features_per_subset\n    end_idx = (i + 1) * features_per_subset if i < 4 else total_features\n    subset_features = X_train.iloc[:, start_idx:end_idx]\n    all_train_subset = pd.concat([subset_features, y_train], axis=1)\n    globals()[f\"all_train_{i + 1}\"] = all_train_subset'''\n\n#print(all_train_splits[0].head())\nplt.figure(figsize=(16, 8))\ncorrelations = {}\n\n'''def feature_stats(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])\n    target = dataset[targ]\n\n    # Calculate feature correlation with the target\n    feature_correlation = features.corrwith(target)\n\n    # Filter features based on correlation threshold\n    selected_features = feature_correlation[abs(feature_correlation) >= correlation_threshold]\n\n    # Create a DataFrame to store the results\n    stats_df = pd.DataFrame({'Correlation': selected_features})\n    print('###############')\n    print(selected_features)\n    print('###############')\n\n    return stats_df\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint(stats_all_train_splits_0)'''\n\nfrom scipy.stats import pearsonr\n\ndef feature_stats(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])\n    target = dataset[targ]\n\n    # Calculate feature correlation with the target and p-values\n    correlation_results = [pearsonr(features[feature], target) for feature in features.columns]\n\n    # Separate positive and negative correlations based on the threshold\n    selected_positive = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr >= correlation_threshold]\n    selected_negative = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr <= -correlation_threshold]\n\n    # Create a DataFrame to store the results\n    stats_df = pd.DataFrame({\n        'Feature': [item[0] for item in selected_positive + selected_negative],\n        'Correlation': [item[1] for item in selected_positive + selected_negative],\n        'P-Value': [item[2] for item in selected_positive + selected_negative],\n    })\n\n    return stats_df\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint('###############')\nprint(stats_all_train_splits_0)\nprint('###############')\n\n\ndef plot_feature_heatmap(dataset, targ, correlation_threshold=0.1):\n    features = dataset.drop(columns=[targ])  \n    target = dataset[targ]\n    feature_correlation = features.corrwith(target)\n    selected_features = feature_correlation[(feature_correlation >= correlation_threshold) | (feature_correlation <= -correlation_threshold)]\n    corr_df = pd.DataFrame({'Correlation': selected_features})\n    if not corr_df.empty:\n        plt.figure(figsize=(50, 20))\n        sns.heatmap(corr_df.transpose(), annot=True, cmap='coolwarm', linewidths=.5)\n        plt.title('Feature Correlation Heatmap')\n        plt.show()\n    else:\n        print(\"No features meet the correlation threshold criteria.\")\n\nplot_feature_heatmap(all_train_splits[0], 'relevance_variance')\n\n'''for column in X_train.columns:\n    if X_train[column].nunique() == 2:  \n        plt.subplot(1, 2, 1)\n        sns.countplot(x=column, data=all_train)\n        plt.title(f'Prevalence of {column} being 1')\n\n        plt.subplot(1, 2, 2)\n        correlation = full_dataset[column].corr(y_train)\n        sns.barplot(x=[column], y=[correlation])\n        correlations[column] = correlation\n\nplt.show()\n\n# Display correlation values\nprint(\"Correlation with the target variable:\")\nfor feature, correlation in correlations.items():\n    print(f\"{feature}: {correlation:.2f}\")'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(5):\n    plot_feature_heatmap(all_train_splits[i], 'relevance_variance')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''plt.figure(figsize=(16, 8))\ncorrelations = {}\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'relevance_variance')\nprint(stats_all_train_splits_0)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''all_train = pd.concat([X_train, y_train], axis=1)\n\ndef plot_significance(dataset, target_column, subplot_title):\n    # Create a bar plot for each binary feature\n    correlations = {}\n\n    for column in dataset.columns:\n        if dataset[column].nunique() == 2:\n            plt.subplot(1, 2, 1)\n            sns.countplot(x=column, data=dataset)\n            plt.title(f'Prevalence of {column} being 1')\n\n            plt.subplot(1, 2, 2)\n            correlation = dataset[column].corr(target_column)\n            sns.barplot(x=[column], y=[correlation])\n            correlations[column] = correlation\n\n    plt.suptitle(subplot_title)\n    plt.show()\n\nnum_splits = 5 \n\nall_train_splits = np.array_split(all_train, num_splits)\nfor i, part in enumerate(all_train_splits):\n    globals()[f'all_train_{i + 1}'] = part\n\nfor idx in range(num_splits):\n    # all_train_1 etc\n    current_split = globals()[f'all_train_{idx + 1}']\n    X_part = X_train\n    y_part = y_train\n\n    # Call the plotting function for each split\n    #plot_significance(current_split, y_part, f'Split {idx + 1}')\n\n# If you want to plot a specific split individually, you can do so like this:\n# plot_significance(all_train_1, all_train_1['your_target_column_name'], 'Split 1')'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_significance(all_train_1, all_train_1['relevance_variance'], 'Split 1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#attribute_names = X_train.columns.tolist()\n#print(attribute_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}