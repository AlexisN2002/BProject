{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-19T13:01:22.485350Z","iopub.execute_input":"2023-10-19T13:01:22.485786Z","iopub.status.idle":"2023-10-19T13:01:22.498880Z","shell.execute_reply.started":"2023-10-19T13:01:22.485757Z","shell.execute_reply":"2023-10-19T13:01:22.497515Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\n/kaggle/input/tabular-playground-series-nov-2021/train.csv\n/kaggle/input/tabular-playground-series-nov-2021/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/test.csv')\ntest['target'] = np.NaN\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test]) \n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \nX_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)\nX_check = X_train #for heatmap\nX_train = X_train.drop(['target'], axis=1)\nX_backup = X_train\nX_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\ny_train = all_data[all_data.train_test==1].target\ny_backup = y_train\nX_test = X_test.drop(['target'], axis=1)\nfor column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-19T13:01:22.504382Z","iopub.execute_input":"2023-10-19T13:01:22.505025Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nX_adapt = X_train.drop(['id'], axis=1)\nall_attributes =X_adapt.columns.tolist()\ncolumns_to_normalize = all_attributes\nscaler = MinMaxScaler()\nX_train_scaled = X_train\nX_train_scaled[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n#X_train_scaled = X_train_scaled[:,~np.all(np.isnan(d), axis=0)]\nX_test_scaled = X_test\nX_test_scaled[columns_to_normalize] = scaler.fit_transform(X_test[columns_to_normalize])\n\n#also norm. backup\nX_backup[columns_to_normalize] = scaler.fit_transform(X_backup[columns_to_normalize])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_column = 'target'\n\n# Calculate the correlations between the target and all features\ncorrelations = training.corr()[target_column]\ncorrelations = correlations.drop('target')\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.05) | (correlations <= -0.05)]\nsignificant_features_index = correlations[(correlations >= 0.05) | (correlations <= -0.05)].index\n\n# Visualize the correlation values\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()\nX_train_sig = X_train_scaled[significant_features_index]\nX_test_sig = X_test_scaled[significant_features_index]\nprint(X_train_sig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n'''num_samples = int(len(X_train_sig) * 0.1)\nrandom_indices = np.random.choice(len(X_train_sig), num_samples, replace=False)\nX_subset = X_train_sig.iloc[random_indices]\ny_subset = y_train.iloc[random_indices]\nX_train, X_val, y_train, y_val = train_test_split(X_subset, y_subset, test_size=0.2, random_state=1)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''num_samples_1 = int(len(X_train_sig) * 0.1)\nnum_samples_2 = int(len(X_train_sig) * 0.3)\nnum_samples_3 = int(len(X_train_sig) * 0.5)\nrandom_indices = np.random.choice(len(X_train_sig), num_samples, replace=False)\nX_subset_1 = X_train_sig.iloc[num_samples_1]\ny_subset_1 = y_train.iloc[num_samples_1]\nX_subset_2 = X_train_sig.iloc[num_samples_2]\ny_subset_2 = y_train.iloc[num_samples_2]\nX_subset_3 = X_train_sig.iloc[num_samples_3]\ny_subset_3 = y_train.iloc[num_samples_3]'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\n# Get the current date and time\ncurrent_time = datetime.datetime.now()\n\n# Convert the current time to a string with a specific format\nformatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Create a string with the formatted time\ntime = f\"{formatted_time}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-learn\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport wandb\n\n'''def create_neural_network():\n    model = MLPClassifier(\n        hidden_layer_sizes=(100,),\n        max_iter=200,\n        alpha=1e-4,\n        solver='adam',\n        activation='relu',\n        learning_rate_init=0.001,\n        random_state=1\n    )\n    return model\n\nwandb.init(project='KaggleNovemer2021NN', name='different NN'+time)\n\n# Define hyperparameters\nmodel = create_neural_network()\n\nmodel.fit(X_train, y_train)\n\n# Log the model's performance\nwandb.log({\"accuracy\": model.score(X_val, y_val)})\n\n# Finish the run\nwandb.finish()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='KaggleNovemer2021NN', name='Testing different sizes of subsets '+time)\n#for HPO\nparam_grid = {\n    'hidden_layer_sizes': [(100,), (150,), (200,)],\n    'max_iter': [300, 500, 400],\n    'alpha': [1e-3, 1e-4, 1e-5],\n    'solver': ['adam'],  \n    'activation': ['relu', 'tanh'],\n    'learning_rate_init': [0.001, 0.01, 0.005]\n}\n\n'''param_grid = {\n    'hidden_layer_sizes': [(150,)],\n    'max_iter': [300],\n    'alpha': [1e-5],\n    'solver': ['adam'],  \n    'activation': ['relu'],\n    'learning_rate_init': [0.005]\n}'''\n\n# Create the neural network model\nmodel = MLPClassifier(random_state=1)\n\n# Perform hyperparameter tuning using GridSearchCV\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_sig)\nprint(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range (1,3)\n    k = 0.2 * i\n    num_samples = int(len(X_train_sig) * 0.2)\n    random_indices = np.random.choice(len(X_train_sig), num_samples, replace=False)\n    X_subset = X_train_sig.iloc[random_indices]\n    y_subset = y_train.iloc[random_indices]\n    X_train_, X_val_, y_train_, y_val_ = train_test_split(X_subset, y_subset, test_size=0.2, random_state=1)\n    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, scoring='roc_auc')\n    grid_search.fit(X_train_, y_train_)\n\n    # Log the best hyperparameters and corresponding accuracy\n    best_params = grid_search.best_params_\n    best_roc_auc = grid_search.best_score_\n    param_list = [(key, value) for key, value in best_params.items()]\n    wandb.config.update(dict(param_list))\n    wandb.log({\"best_roc_auc\": best_roc_auc})\n    # Fit the model with the best hyperparameters\n    best_model = MLPClassifier(**best_params, random_state=1)\n    best_model.fit(X_train_, y_train_)\n\n    test_accuracy = best_model.score(X_val_, y_val_)\n    wandb.log({\"test_accuracy with subset size \"+str(k): test_accuracy})\n    k = 0.2\n\n# Finish the run\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}