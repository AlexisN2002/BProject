{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sb\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-21T14:55:26.956064Z","iopub.execute_input":"2023-07-21T14:55:26.956637Z","iopub.status.idle":"2023-07-21T14:55:26.968377Z","shell.execute_reply.started":"2023-07-21T14:55:26.956578Z","shell.execute_reply":"2023-07-21T14:55:26.967393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ndata=pd.read_csv('/kaggle/input/titanic/train.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\n\n%matplotlib inline\nall_data.columns","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:26.972956Z","iopub.execute_input":"2023-07-21T14:55:26.973471Z","iopub.status.idle":"2023-07-21T14:55:27.020071Z","shell.execute_reply.started":"2023-07-21T14:55:26.973430Z","shell.execute_reply":"2023-07-21T14:55:27.018852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualizing the Data**\nI used the amazing data rep by https://www.kaggle.com/code/hardikarora24/titanic0\n\n\n","metadata":{}},{"cell_type":"code","source":"training.Survived.value_counts().plot.pie(autopct='%1.2f%%')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:27.021674Z","iopub.execute_input":"2023-07-21T14:55:27.022059Z","iopub.status.idle":"2023-07-21T14:55:27.180161Z","shell.execute_reply.started":"2023-07-21T14:55:27.022027Z","shell.execute_reply":"2023-07-21T14:55:27.178581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.countplot(training, x = 'Sex', hue = 'Survived')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:27.184819Z","iopub.execute_input":"2023-07-21T14:55:27.185525Z","iopub.status.idle":"2023-07-21T14:55:27.506082Z","shell.execute_reply.started":"2023-07-21T14:55:27.185466Z","shell.execute_reply":"2023-07-21T14:55:27.505147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.countplot(training, x = 'Embarked', hue='Survived')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:27.507567Z","iopub.execute_input":"2023-07-21T14:55:27.508105Z","iopub.status.idle":"2023-07-21T14:55:27.838975Z","shell.execute_reply.started":"2023-07-21T14:55:27.508073Z","shell.execute_reply":"2023-07-21T14:55:27.837633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.countplot(training, x = 'Survived', hue = 'Pclass')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:27.840508Z","iopub.execute_input":"2023-07-21T14:55:27.840895Z","iopub.status.idle":"2023-07-21T14:55:28.176742Z","shell.execute_reply.started":"2023-07-21T14:55:27.840864Z","shell.execute_reply":"2023-07-21T14:55:28.175459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.crosstab(training.Pclass,training.Survived,margins=True).style.background_gradient(cmap='cool')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:28.178353Z","iopub.execute_input":"2023-07-21T14:55:28.178772Z","iopub.status.idle":"2023-07-21T14:55:28.238121Z","shell.execute_reply.started":"2023-07-21T14:55:28.178737Z","shell.execute_reply":"2023-07-21T14:55:28.236810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.countplot(training, x = 'Parch', hue = 'Survived')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:28.240039Z","iopub.execute_input":"2023-07-21T14:55:28.240742Z","iopub.status.idle":"2023-07-21T14:55:28.598916Z","shell.execute_reply.started":"2023-07-21T14:55:28.240706Z","shell.execute_reply":"2023-07-21T14:55:28.597631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.countplot(training, x = 'SibSp', hue = 'Survived')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:28.604301Z","iopub.execute_input":"2023-07-21T14:55:28.604748Z","iopub.status.idle":"2023-07-21T14:55:28.983061Z","shell.execute_reply.started":"2023-07-21T14:55:28.604713Z","shell.execute_reply":"2023-07-21T14:55:28.982218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sb.histplot(training, x='Age', hue='Survived', alpha=0.6, bins=25)\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:28.984384Z","iopub.execute_input":"2023-07-21T14:55:28.985191Z","iopub.status.idle":"2023-07-21T14:55:29.394308Z","shell.execute_reply.started":"2023-07-21T14:55:28.985155Z","shell.execute_reply":"2023-07-21T14:55:29.393154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,3,figsize=(18,8))\nsb.violinplot(x=\"Pclass\",y=\"Age\", hue=\"Survived\", data=training,split=True,ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsb.violinplot(x=\"Sex\",y=\"Age\", hue=\"Survived\", data=training,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nsb.violinplot(x=\"Embarked\",y=\"Age\", hue=\"Survived\", data=training,split=True,ax=ax[2])\nax[2].set_title('Embarked and Age vs Survived')\nax[2].set_yticks(range(0,110,10))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:29.396168Z","iopub.execute_input":"2023-07-21T14:55:29.396954Z","iopub.status.idle":"2023-07-21T14:55:30.614995Z","shell.execute_reply.started":"2023-07-21T14:55:29.396912Z","shell.execute_reply":"2023-07-21T14:55:30.613799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:30.616914Z","iopub.execute_input":"2023-07-21T14:55:30.617380Z","iopub.status.idle":"2023-07-21T14:55:30.638777Z","shell.execute_reply.started":"2023-07-21T14:55:30.617343Z","shell.execute_reply":"2023-07-21T14:55:30.637572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:30.640246Z","iopub.execute_input":"2023-07-21T14:55:30.641237Z","iopub.status.idle":"2023-07-21T14:55:30.688819Z","shell.execute_reply.started":"2023-07-21T14:55:30.641195Z","shell.execute_reply":"2023-07-21T14:55:30.687497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.describe()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:30.690506Z","iopub.execute_input":"2023-07-21T14:55:30.690845Z","iopub.status.idle":"2023-07-21T14:55:30.731686Z","shell.execute_reply.started":"2023-07-21T14:55:30.690817Z","shell.execute_reply":"2023-07-21T14:55:30.730589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training['Died']=1- training['Survived']\ntraining.groupby('Sex')[['Survived','Died']].sum().plot(kind='bar',figsize=(30, 7))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:30.733082Z","iopub.execute_input":"2023-07-21T14:55:30.733924Z","iopub.status.idle":"2023-07-21T14:55:31.142902Z","shell.execute_reply.started":"2023-07-21T14:55:30.733887Z","shell.execute_reply":"2023-07-21T14:55:31.141779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Project Planning:**\n\nFor successfull Machine Learning, Data processing is a vital part of the project. Giving the model good data to work with will be the first major step.\nFirstly, cleaning up the data and then doing some feature engineering\n","metadata":{}},{"cell_type":"markdown","source":"Firstly let's look at possible categories to lump together\n(I lost my progess of my first day of work due to Kaggle not saving my notebook, hence I will recap what I did but not go into as much detail)\n","metadata":{}},{"cell_type":"code","source":"print(pd.pivot_table(training, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint('---------------------------------')\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint('---------------------------------')\nprint(pd.pivot_table(training, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))\ntraining['CabinComb'] = training.Cabin.apply(lambda x: str(x)[0])\nprint('---------------------------------')\nprint(training.CabinComb.value_counts())\npd.pivot_table(training,index='Survived',columns='CabinComb', values = 'Name', aggfunc='count')\nprint('---------------------------------')\n\ni = 0\nj = 0\ntrain = training.sort_values(by='Cabin')\nwhile i <= 195:\n    j = i\n    i += 15\n    train['Cabin' + str(i)] = train['Cabin'].iloc[j:i]\n    print(pd.pivot_table(train,index='Survived',columns='Cabin' + str(i), values = 'Name', aggfunc='count'))\n    print('---------------------------------')\n\n\ndef extract_cabin_num(cabin):\n    if isinstance(cabin, str):\n        cabin_num = cabin.split()[0][1:]  \n        if cabin_num.isdigit():\n            return int(cabin_num)\n    return None\n\ndef categorize_cabin_smaller_33(cabin_num):\n    if cabin_num is not None and cabin_num < 33:\n        return 'Yes'\n    return 'No'\n\ndef categorize_cabin_33_66(cabin_num):\n    if cabin_num is not None and 33 <= cabin_num <= 66:\n        return 'Yes'\n    return 'No'\n\ndef categorize_cabin_bigger_66(cabin_num):\n    if cabin_num is not None and cabin_num > 66:\n        return 'Yes'\n    return 'No'\n\ntraining['CabinNum'] = training['Cabin'].map(extract_cabin_num)\n\ntraining['Cabin_0_33'] = training['CabinNum'].map(categorize_cabin_smaller_33)\ntraining['Cabin_33_66'] = training['CabinNum'].map(categorize_cabin_33_66)\ntraining['Cabin_66'] = training['CabinNum'].map(categorize_cabin_bigger_66)\n\n\nprint(pd.pivot_table(training,index='Survived',columns='Cabin_0_33', values = 'Name', aggfunc='count'))\nprint('---------------------------------')\nprint(pd.pivot_table(training,index='Survived',columns='Cabin_33_66', values = 'Name', aggfunc='count'))\nprint('---------------------------------')\nprint(pd.pivot_table(training,index='Survived',columns='Cabin_66', values = 'Name', aggfunc='count'))\nprint('---------------------------------')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:31.144829Z","iopub.execute_input":"2023-07-21T14:55:31.145187Z","iopub.status.idle":"2023-07-21T14:55:31.421403Z","shell.execute_reply.started":"2023-07-21T14:55:31.145158Z","shell.execute_reply":"2023-07-21T14:55:31.420330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look for some notable trends in the rooms by number\n","metadata":{}},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsb.violinplot(x=\"CabinComb\",y=\"CabinNum\", hue=\"Survived\", data=training,split=True,ax=ax[0])\nax[0].set_title('Cabin_0_33 and Age vs Survived')\nax[0].set_yticks(range(0,150,20))\n\nsb.violinplot(x=\"Cabin_0_33\",y=\"CabinNum\", hue=\"Survived\", data=training,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\n#ax[1].set_yticks(range(0,110,10))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:31.422857Z","iopub.execute_input":"2023-07-21T14:55:31.423791Z","iopub.status.idle":"2023-07-21T14:55:32.505628Z","shell.execute_reply.started":"2023-07-21T14:55:31.423758Z","shell.execute_reply":"2023-07-21T14:55:32.504762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsb.violinplot(x=\"Cabin_33_66\",y=\"CabinNum\", hue=\"Survived\", data=training,split=True,ax=ax[0])\nax[0].set_title('Cabin_0_33 and Age vs Survived')\nax[0].set_yticks(range(0,150,20))\n\nsb.violinplot(x=\"Cabin_66\",y=\"CabinNum\", hue=\"Survived\", data=training,split=True,ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\n#ax[1].set_yticks(range(0,110,10))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:32.506797Z","iopub.execute_input":"2023-07-21T14:55:32.507344Z","iopub.status.idle":"2023-07-21T14:55:33.376343Z","shell.execute_reply.started":"2023-07-21T14:55:32.507308Z","shell.execute_reply":"2023-07-21T14:55:33.375271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''def extract_cabin_char(cabin):\n    if isinstance(cabin, str):\n        cabin_char = cabin[0]\n        if cabin_char.isstring():\n            return str(cabin_char)\n    return None\n\ndef extract_danger_zone_E(cabin):\n    if cabin_char == 'E':\n        if cabin_num is not None and cabin_num > 40:\n            if cabin_num < 80:\n                return 'Yes'\n    return 'No' '''\n    \n#training['cabin_char'] = training['Cabin'].map(extract_cabin_char)\n#training['Danger_Zone_E'] = training['Cabin'].map(extract_danger_zone_E)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.378028Z","iopub.execute_input":"2023-07-21T14:55:33.378382Z","iopub.status.idle":"2023-07-21T14:55:33.386078Z","shell.execute_reply.started":"2023-07-21T14:55:33.378352Z","shell.execute_reply":"2023-07-21T14:55:33.384739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabins sharing a letter can be combined as they share similar postions on the ship like a level. Further I tried finding a conncection between the numbers. For example if having a low number (like < 33 as an arbitrary example I picked) would represent being closer to an exit and thus maybe a lifeboat was an indication of survival.\nIt turns out, cabin numbers between E25 and E75 had a particularly low chance of survival.","metadata":{}},{"cell_type":"code","source":"training['numeric_ticket'] = training.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntraining['ticket_letters'] = training.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\ntraining['numeric_ticket'].value_counts()\ntraining['ticket_letters'].value_counts()\nprint(pd.pivot_table(training,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count'))\nprint(pd.pivot_table(training,index='Survived',columns='ticket_letters', values = 'Ticket', aggfunc='count'))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.387607Z","iopub.execute_input":"2023-07-21T14:55:33.387942Z","iopub.status.idle":"2023-07-21T14:55:33.436548Z","shell.execute_reply.started":"2023-07-21T14:55:33.387912Z","shell.execute_reply":"2023-07-21T14:55:33.435355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I also thought the tickets having a number or not would give some information, like being the cabins of special passengers who may have a particularly good/bad postion on the ship. Because of losing my code I used https://www.kaggle.com/code/kenjee/titanic-project-example 's lambda function for the ticket letters.\nI wasn't sure however if I could combine certain special ticket types so I decided against it for the moment.\n","metadata":{}},{"cell_type":"markdown","source":"Next I was interested in weather the titles of the passengers would make a difference, like \"nobles\" being put together if they all died or suvived no matter their individual title or women being put together. \n\nIt turns out that 'nobles' (men and women) actually had a survival rate higher their non-noble counterparts, especially for men.\n\nFor women, their marital status plays a role int their survival rate. Unmarried women (Miss, Mlle, Mme) having a lower surival rate than married ones (Mrs) so I decided to combine them based on that.  ","metadata":{}},{"cell_type":"code","source":"training['title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntraining['title'].value_counts()\n\n\nprint(pd.pivot_table(training,index='Survived',columns='title', values = 'Ticket', aggfunc='count'))\n\n#training['unmarried_women'] = training['Miss'].combine_first(training['Mlle'])\n#training['married_women'] = ","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.437785Z","iopub.execute_input":"2023-07-21T14:55:33.438103Z","iopub.status.idle":"2023-07-21T14:55:33.465314Z","shell.execute_reply.started":"2023-07-21T14:55:33.438076Z","shell.execute_reply":"2023-07-21T14:55:33.463908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.466732Z","iopub.execute_input":"2023-07-21T14:55:33.467061Z","iopub.status.idle":"2023-07-21T14:55:33.495061Z","shell.execute_reply.started":"2023-07-21T14:55:33.467031Z","shell.execute_reply":"2023-07-21T14:55:33.493808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training['title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntraining.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.496057Z","iopub.execute_input":"2023-07-21T14:55:33.496356Z","iopub.status.idle":"2023-07-21T14:55:33.525621Z","shell.execute_reply.started":"2023-07-21T14:55:33.496330Z","shell.execute_reply":"2023-07-21T14:55:33.524502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#training['Miss'] = training['title'].apply(lambda x: 'Yes' if x == 'Miss' else 'No')\n#training['Mlle'] = training['title'].apply(lambda x: 'Yes' if x == 'Mlle' else 'No')\ntitle_dummies = pd.get_dummies(training['title'])\n\n'''training_dummies = pd.concat([training, title_dummies], axis=1)\n#training['unmarried_women'] = training['Miss'].combine_first(training['Mlle'])\n#training['unmarried_women'] = training['title'].apply(lambda x: 'unmarried_women' if x in ['Miss', 'Mlle'] else 'No')\ntraining['title'] = training_dummies['title'].replace(['Miss', 'Mlle'], 'unmarried_women')\ntraining['title'] = training_dummies['title'].replace(['Mme', 'Mrs'], 'married_women')\ntraining['title'] = training_dummies['title'].replace(['Master', 'Col', 'Sir', 'Major', 'Jonkheer'], 'noblemen')\ntraining['title'] = training_dummies['title'].replace(['the Countess', 'Lady'], 'noblewomen')\ntraining['title'] = training_dummies['title'].replace(['Mr', 'Don'], 'men')'''\n\ntraining['title'] = training['title'].replace(['Miss', 'Mlle'], 'unmarried_women')\ntraining['title'] = training['title'].replace(['Mme', 'Mrs'], 'married_women')\ntraining['title'] = training['title'].replace(['Master', 'Col', 'Sir', 'Major', 'Jonkheer'], 'noblemen')\ntraining['title'] = training['title'].replace(['the Countess', 'Lady'], 'noblewomen')\ntraining['title'] = training['title'].replace(['Mr', 'Don'], 'men')\n\n# Calculate survival rates based on the assigned titles\nsurvival_rates = training.groupby('title')['Survived'].mean().reset_index()\n\nplt.figure(figsize=(10, 6))\n\n# Plot the survival rates using Seaborn\nsb.barplot(data=survival_rates, x='title', y='Survived')\nplt.xlabel('Title')\nplt.ylabel('Survival Rate')\nplt.title('Survival Rates by Title')\nplt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\nplt.show()\n#training['unmarried_women'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.532486Z","iopub.execute_input":"2023-07-21T14:55:33.532871Z","iopub.status.idle":"2023-07-21T14:55:33.889090Z","shell.execute_reply.started":"2023-07-21T14:55:33.532841Z","shell.execute_reply":"2023-07-21T14:55:33.887769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.890625Z","iopub.execute_input":"2023-07-21T14:55:33.891005Z","iopub.status.idle":"2023-07-21T14:55:33.921823Z","shell.execute_reply.started":"2023-07-21T14:55:33.890971Z","shell.execute_reply":"2023-07-21T14:55:33.920549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excluded_title = 'men'\nfiltered_data = training[training['title'] != excluded_title]\ntitle_pclass_counts = filtered_data.groupby(['title', 'Pclass']).size().reset_index(name='Count')\n\n# Set the figure size\nplt.figure(figsize=(10, 6))\nsb.barplot(data=title_pclass_counts, x='title', y='Count', hue='Pclass')\nplt.xlabel('Title')\nplt.ylabel('Count')\nplt.title('Number of People in Each Pclass by Title')\nplt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\nplt.legend(title='Pclass')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:33.924018Z","iopub.execute_input":"2023-07-21T14:55:33.924511Z","iopub.status.idle":"2023-07-21T14:55:34.516728Z","shell.execute_reply.started":"2023-07-21T14:55:33.924464Z","shell.execute_reply":"2023-07-21T14:55:34.515441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\ntitle_pclass_counts = filtered_data.groupby(['title', 'Pclass']).size().reset_index(name='Count')\nplt.figure(figsize=(12, 8))\n\n# Get the unique titles\ntitles = title_pclass_counts['title'].unique()\n\n# Count the number of people in each title-Pclass combination\ntitle_pclass_counts = training.groupby(['title', 'Pclass']).size().reset_index(name='Count')\n\n# Set the figure size\nplt.figure(figsize=(12, 8))\n\n# Get the unique titles\ntitles = title_pclass_counts['title'].unique()\n\nnum_titles = len(titles)\nnum_cols = 3\nnum_rows = math.ceil(num_titles / num_cols)\n\n# Set the figure size\nplt.figure(figsize=(12, 4 * num_rows))\n\n# Create a pie chart for each title\nfor i, title in enumerate(titles):\n    data = title_pclass_counts[title_pclass_counts['title'] == title]\n    sizes = data['Count']\n    labels = data['Pclass']\n\n    ax = plt.subplot(num_rows, num_cols, i + 1)\n    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n    ax.axis('equal')\n    ax.set_title(f'{title} - Pclass Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:34.518341Z","iopub.execute_input":"2023-07-21T14:55:34.518752Z","iopub.status.idle":"2023-07-21T14:55:36.125959Z","shell.execute_reply.started":"2023-07-21T14:55:34.518719Z","shell.execute_reply":"2023-07-21T14:55:36.124822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"survival_rate = training.groupby(['Pclass', 'title']).Survived.mean().reset_index()\nplt.figure(figsize=(12, 8))\n\n# Create a stacked bar chart\nsb.barplot(data=survival_rate, x='Pclass', y='Survived', hue='title')\nplt.xlabel('Pclass')\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Pclass and Title')\n\nplt.legend(title='Title', loc='best')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:36.127255Z","iopub.execute_input":"2023-07-21T14:55:36.127582Z","iopub.status.idle":"2023-07-21T14:55:36.721300Z","shell.execute_reply.started":"2023-07-21T14:55:36.127553Z","shell.execute_reply":"2023-07-21T14:55:36.719362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = pd.read_csv('/kaggle/input/titanic/train.csv')\ntrain_see = k\ntrain_see['CabinNum'] = train_see['Cabin'].map(extract_cabin_num)\ntrain_see['numeric_ticket'] = train_see.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain_see['ticket_letters'] = train_see.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\ntrain_see.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)\ncorr_matrix = train_see.corr()\n\nplt.figure(figsize=(10, 8))\nsb.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:36.722783Z","iopub.execute_input":"2023-07-21T14:55:36.723106Z","iopub.status.idle":"2023-07-21T14:55:37.264147Z","shell.execute_reply.started":"2023-07-21T14:55:36.723078Z","shell.execute_reply":"2023-07-21T14:55:37.262987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#create all categorical variables that we did above for both training and test sets\ntraining = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\n\nall_data = pd.concat([training,test])\n\nall_data['CabinNum'] = all_data['Cabin'].map(extract_cabin_num)\nall_data['CabinChar'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\nall_data['CabinNum'] = all_data['Cabin'].map(extract_cabin_num)\n\nall_data['Cabin_Lo'] = all_data['CabinNum'].map(categorize_cabin_smaller_33)\nall_data['Cabin_Mid'] = all_data['CabinNum'].map(categorize_cabin_33_66)\nall_data['Cabin_Up'] = all_data['CabinNum'].map(categorize_cabin_bigger_66)\n\nall_data['title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\nall_data_dummies = pd.get_dummies(all_data['title'])\nall_data_dummies = pd.concat([training, title_dummies], axis=1)\nall_data['title'] = all_data['title'].replace(['Miss', 'Mlle'], 'unmarried_women')\nall_data['title'] = all_data['title'].replace(['Mme', 'Mrs'], 'married_women')\nall_data['title'] = all_data['title'].replace(['Master', 'Col', 'Sir', 'Major', 'Jonkheer'], 'noblemen')\nall_data['title'] = all_data['title'].replace(['the Countess', 'Lady'], 'noblewomen')\nall_data['title'] = all_data['title'].replace(['Mr', 'Don'], 'men')\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(training.Age.mean())\nall_data.Age = all_data.Age.fillna(all_data.Age.median())\n#all_data.Fare = all_data.Fare.fillna(training.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(all_data.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\nall_data.dropna(subset=['CabinNum'],inplace = True)\n\n#tried log norm of sibsp (not used)\n#all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\n#all_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(training.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','CabinNum',\n                                    'norm_fare','Embarked','CabinChar','numeric_ticket','title',\n                                    'Cabin_Lo', 'Cabin_Mid', 'Cabin_Up', 'train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:37.265847Z","iopub.execute_input":"2023-07-21T14:55:37.266360Z","iopub.status.idle":"2023-07-21T14:55:37.611229Z","shell.execute_reply.started":"2023-07-21T14:55:37.266330Z","shell.execute_reply":"2023-07-21T14:55:37.610065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:37.613553Z","iopub.execute_input":"2023-07-21T14:55:37.613887Z","iopub.status.idle":"2023-07-21T14:55:37.632716Z","shell.execute_reply.started":"2023-07-21T14:55:37.613858Z","shell.execute_reply":"2023-07-21T14:55:37.631710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:37.634625Z","iopub.execute_input":"2023-07-21T14:55:37.635035Z","iopub.status.idle":"2023-07-21T14:55:37.644228Z","shell.execute_reply.started":"2023-07-21T14:55:37.634998Z","shell.execute_reply":"2023-07-21T14:55:37.643283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" gnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:37.645159Z","iopub.execute_input":"2023-07-21T14:55:37.645483Z","iopub.status.idle":"2023-07-21T14:55:37.695020Z","shell.execute_reply.started":"2023-07-21T14:55:37.645455Z","shell.execute_reply":"2023-07-21T14:55:37.694098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:37.696356Z","iopub.execute_input":"2023-07-21T14:55:37.696704Z","iopub.status.idle":"2023-07-21T14:55:38.098288Z","shell.execute_reply.started":"2023-07-21T14:55:37.696675Z","shell.execute_reply":"2023-07-21T14:55:38.097267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=7)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:38.099454Z","iopub.execute_input":"2023-07-21T14:55:38.099846Z","iopub.status.idle":"2023-07-21T14:55:38.175387Z","shell.execute_reply.started":"2023-07-21T14:55:38.099814Z","shell.execute_reply":"2023-07-21T14:55:38.174019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=7)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:38.177074Z","iopub.execute_input":"2023-07-21T14:55:38.177459Z","iopub.status.idle":"2023-07-21T14:55:38.301319Z","shell.execute_reply.started":"2023-07-21T14:55:38.177425Z","shell.execute_reply":"2023-07-21T14:55:38.300095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"a","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('dt',dt),('gnb',gnb),('svc',svc)], voting = 'soft') \ncv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:38.303073Z","iopub.execute_input":"2023-07-21T14:55:38.303577Z","iopub.status.idle":"2023-07-21T14:55:38.743564Z","shell.execute_reply.started":"2023-07-21T14:55:38.303522Z","shell.execute_reply":"2023-07-21T14:55:38.742186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:38.746723Z","iopub.execute_input":"2023-07-21T14:55:38.747412Z","iopub.status.idle":"2023-07-21T14:55:38.754786Z","shell.execute_reply.started":"2023-07-21T14:55:38.747376Z","shell.execute_reply":"2023-07-21T14:55:38.753384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [1500],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:55:38.756712Z","iopub.execute_input":"2023-07-21T14:55:38.757214Z","iopub.status.idle":"2023-07-21T14:55:39.472787Z","shell.execute_reply.started":"2023-07-21T14:55:38.757167Z","shell.execute_reply":"2023-07-21T14:55:39.471941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T15:00:06.925132Z","iopub.execute_input":"2023-07-21T15:00:06.925683Z","iopub.status.idle":"2023-07-21T15:05:24.283982Z","shell.execute_reply.started":"2023-07-21T15:00:06.925643Z","shell.execute_reply":"2023-07-21T15:05:24.282470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')'''","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.245181Z","iopub.status.idle":"2023-07-21T14:59:25.245823Z","shell.execute_reply.started":"2023-07-21T14:59:25.245488Z","shell.execute_reply":"2023-07-21T14:59:25.245516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.247365Z","iopub.status.idle":"2023-07-21T14:59:25.247806Z","shell.execute_reply.started":"2023-07-21T14:59:25.247599Z","shell.execute_reply":"2023-07-21T14:59:25.247618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.249241Z","iopub.status.idle":"2023-07-21T14:59:25.249681Z","shell.execute_reply.started":"2023-07-21T14:59:25.249468Z","shell.execute_reply":"2023-07-21T14:59:25.249487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras \nfrom keras.models import Sequential # intitialize the ANN\nfrom keras.layers import Dense      # create layers\nmodel = Sequential()\n\n# layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 35))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the ANN\nmodel.fit(X_train, y_train, batch_size = 32, epochs = 200)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T15:08:53.316325Z","iopub.execute_input":"2023-07-21T15:08:53.316765Z","iopub.status.idle":"2023-07-21T15:08:59.610754Z","shell.execute_reply.started":"2023-07-21T15:08:53.316733Z","shell.execute_reply":"2023-07-21T15:08:59.609561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From doing some testing myself and looking at what GMs are doing, certain trends became apparent:\ngenerally, the same tried and true models are used, their parameters tuned in a similar way, even for models with dozens of tune-able parameters, and ensenmbled using one of the few options.\n\ncertain optimal parmeters need to be found either through grid or random search, there seems to be a consensus among GMs that one should do grid search. This is what I could gather for some models:\nIt is worth noting that I couldn't find out yet weather or not this is something general across all competitions or not.\n\nExtra Trees:\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nRandom Forrest:\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nGB classifiers:\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\nSVM:\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}","metadata":{}},{"cell_type":"markdown","source":"Sometimes, instead of searching parameters with grid or random search, Out Of Fold is used to get the most important features \n\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\n\nand then use stacking like:\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\nas a first level model. As one GM noted and what I could also see is in his words: \"There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores.\" -ANISOTROPIC\n\nas second level learning, the XGBoost Library is then used, apparently to great effect.\n\nFrom my study of GMs and their techniques, there is a certain consensus, either through a sort of \"industry standard\" or \"competition standard\" way or through a lot of hidden expertise among GMs of tuning HPs and which one's to pick.\nBecause AutoML tools are meant to be used by novices or people who lack experience with ML, I think tools need to be included in AutoML Libraries for the user to find this out.\n\nFurther it seems that Ensembling is a vital part, one might even say at the heart of most final submissions by GMs. But here, which ensembling method to pick seems to matter to GMs a lot and they are particular from the outset which one to pick. Again through a hidden expertise a novice is lacking, AutoML libraries should find a way to tackle this.\n\nLastly, a more rare use in Kaggle, or at least Titanic, but certainly another vital tool in every ML engineers arsenal is the Neural network. For which suprisingly little existing easy architecture exists. The existing one having a high barrier to entry, again not suitable for a novice. Further as my personal idea, an easy way to visualize the NN one has constructed without having to get super familiar with something like pandas and using it for a different purpose than it was intended for.","metadata":{}},{"cell_type":"markdown","source":"**Feature Engineering**\n","metadata":{}},{"cell_type":"code","source":"'''test['CabinNum'] = test['Cabin'].map(extract_cabin_num)\ntest['numeric_ticket'] = test.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntest['ticket_letters'] = test.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\ntest['title'] = training.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntitle_dummies = pd.get_dummies(test['title'])\ntest_dummies = pd.concat([training, title_dummies], axis=1)\ntest['title'] = test_dummies['title'].replace(['Miss', 'Mlle'], 'unmarried_women')\ntest['title'] = test_dummies['title'].replace(['Mme', 'Mrs'], 'married_women')\ntest['title'] = test_dummies['title'].replace(['Master', 'Col', 'Sir', 'Major', 'Jonkheer'], 'noblemen')\ntest['title'] = test_dummies['title'].replace(['the Countess', 'Lady'], 'noblewomen')\ntest['title'] = test_dummies['title'].replace(['Mr', 'Don'], 'men')\n\ntitle_dummies = pd.get_dummies(test['title'])\ntest_dummies = pd.concat([test, title_dummies], axis=1)\ntest['title'] = test_dummies['title'].replace(['Miss', 'Mlle'], 'unmarried_women')\ntest['title'] = test['title'].replace(['Mme', 'Mrs'], 'married_women')\ntest['title'] = test['title'].replace(['Master', 'Col', 'Sir', 'Major', 'Jonkheer'], 'noblemen')\ntest['title'] = test['title'].replace(['the Countess', 'Lady'], 'noblewomen')\ntest['title'] = test['title'].replace(['Mr', 'Don'], 'men')\n'''","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.252854Z","iopub.status.idle":"2023-07-21T14:59:25.253333Z","shell.execute_reply.started":"2023-07-21T14:59:25.253122Z","shell.execute_reply":"2023-07-21T14:59:25.253142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.254157Z","iopub.status.idle":"2023-07-21T14:59:25.254525Z","shell.execute_reply.started":"2023-07-21T14:59:25.254344Z","shell.execute_reply":"2023-07-21T14:59:25.254361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''from sklearn.model_selection import train_test_split\n\nX = training.drop('Survived', axis = 1)\n#X = data\nY = training['Survived']\n#Y = data\n\nX_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_training[['Age']] = scaler.fit_transform(X_training[['Age']])\nX_test[['Age']] = scaler.fit_transform(X_test[['Age']])\n\nX_training[['Fare']] = scaler.fit_transform(X_training[['Fare']])\nX_test[['Fare']] = scaler.fit_transform(X_test[['Fare']])\n\nX_training[['Pclass']] = scaler.fit_transform(X_training[['Pclass']])\nX_test[['Pclass']] = scaler.fit_transform(X_test[['Pclass']])\n\nX_training[['SibSp']] = scaler.fit_transform(X_training[['SibSp']])\nX_test[['SibSp']] = scaler.fit_transform(X_test[['SibSp']])\n\nX_training[['Parch']] = scaler.fit_transform(X_training[['Parch']])\nX_test[['Parch']] = scaler.fit_transform(X_test[['Parch']])\n\nX_test.drop(['Sex', 'Fare'], axis = 1, inplace = True)\n#X_test.drop('Sex_male', axis = 1, inplace = True)\n\nX_test.head(5)'''","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:59:25.258772Z","iopub.status.idle":"2023-07-21T14:59:25.259439Z","shell.execute_reply.started":"2023-07-21T14:59:25.259200Z","shell.execute_reply":"2023-07-21T14:59:25.259225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'''from sklearn.model_selection import train_test_split\n\nX = training.drop('Survived', axis = 1)\nY = training['Survived']\nX = training.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked', 'Parch'], axis = 1, inplace = True)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 42)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)'''","metadata":{}},{"cell_type":"markdown","source":"):","metadata":{}},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2023-07-04T19:18:20.507661Z","iopub.status.idle":"2023-07-04T19:18:20.508526Z","shell.execute_reply.started":"2023-07-04T19:18:20.508295Z","shell.execute_reply":"2023-07-04T19:18:20.508317Z"}}}]}