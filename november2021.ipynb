{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-13T13:18:21.134585Z","iopub.execute_input":"2023-10-13T13:18:21.135286Z","iopub.status.idle":"2023-10-13T13:18:21.146146Z","shell.execute_reply.started":"2023-10-13T13:18:21.135245Z","shell.execute_reply":"2023-10-13T13:18:21.144340Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"/kaggle/input/tabular-playground-series-nov-2021/sample_submission.csv\n/kaggle/input/tabular-playground-series-nov-2021/train.csv\n/kaggle/input/tabular-playground-series-nov-2021/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/test.csv')\ntest['target'] = np.NaN\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test]) \n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \nX_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)\nX_check = X_train #for heatmap\nX_train = X_train.drop(['target'], axis=1)\nX_backup = X_train\nX_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\ny_train = all_data[all_data.train_test==1].target\ny_backup = y_train\nX_test = X_test.drop(['target'], axis=1)\nfor column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-13T13:18:21.156079Z","iopub.execute_input":"2023-10-13T13:18:21.156906Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"}]},{"cell_type":"code","source":"training","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Load your data into a DataFrame, assuming your target variable is named 'target'\n# Replace 'your_data.csv' with your actual data file path or source\n\n# Create a countplot to visualize the distribution of the target variable\nsns.set(style=\"whitegrid\")  # Set the style of the plot\n\nplt.figure(figsize=(8, 6))  # Set the figure size (width, height)\nsns.countplot(data=training, x='target')  # Replace 'target' with your actual column name\n\n# Customize the plot\nplt.title('Distribution of Target Variable (0 and 1)')\nplt.xlabel('Target Value')\nplt.ylabel('Count')\n\n# Show the plot\nplt.show()\n#even though it is a task about finding spam emails, the data is balanced","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nX_adapt = X_train.drop(['id'], axis=1)\nall_attributes =X_adapt.columns.tolist()\ncolumns_to_normalize = all_attributes\nscaler = MinMaxScaler()\nX_train_scaled = X_train\nX_train_scaled[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n#X_train_scaled = X_train_scaled[:,~np.all(np.isnan(d), axis=0)]\nX_test_scaled = X_test\nX_test_scaled[columns_to_normalize] = scaler.fit_transform(X_test[columns_to_normalize])\n\n#also norm. backup\nX_backup[columns_to_normalize] = scaler.fit_transform(X_backup[columns_to_normalize])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_scaled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_column = 'target'\n\n# Calculate the correlations between the target and all features\ncorrelations = training.corr()[target_column]\ncorrelations = correlations.drop('target')\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.1) | (correlations <= -0.1)]\nsignificant_features_index = correlations[(correlations >= 0.1) | (correlations <= -0.1)].index\n\n# Visualize the correlation values\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(significant_features_index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_sig = X_train_scaled[significant_features_index]\nprint(X_train_sig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_backup)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_fraction = 0.1\n\nnum_samples = int(len(X_train_sig) * sample_fraction)\nnum_samples_2 = int(len(training) * sample_fraction)\n\nX_train_sig.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\n\nrandom_indices = np.random.choice(len(X_train_sig), num_samples, replace=False)\nrandom_indices_2 = np.random.choice(len(training), num_samples_2, replace=False)\n\nX_subset = X_train_sig.iloc[random_indices]\ny_subset = y_train.iloc[random_indices]\n\nX_backup = X_backup.iloc[random_indices_2]\ny_backup = y_backup.iloc[random_indices_2]\n\n'''X_val = X_train.iloc[random_indices]\ny_val = y_train.iloc[random_indices]'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_subset)\nprint(y_subset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_backup)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n'''wandb.init(project='KaggleVovember2021', name='untuned attemps')\n\ndef run_cross_validation(models, X_train, y_train, cv=5):\n    results = []\n\n    for model in models:\n        model_name = model.__class__.__name__\n\n        if model_name == 'XGBClassifier':\n            class_counts = y_train.value_counts()\n            class_weight = class_counts[0] / class_counts[1]\n            model = XGBClassifier(scale_pos_weight=class_weight, random_state=1)\n        \n        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)\n        \n        # Log the results to WandB:\n        wandb.log({f'{model_name}_CV_Scores': cv_scores.tolist(), f'{model_name}_Mean_CV_Score': np.mean(cv_scores)})\n        \n        results.append({\n            'model_name': model_name,\n            'cv_scores': cv_scores,\n            'mean_cv_score': np.mean(cv_scores),\n            #'parameters': {\n                #'scale_pos_weight': class_weight if model_name == 'XGBClassifier' else None\n            #}\n        })\n\n    return results\n\nxgb = XGBClassifier(random_state =1)\n\nmodels = [\n    xgb,\n    GaussianNB(),\n    LogisticRegression(max_iter=2000),\n    RandomForestClassifier(random_state=1),\n    GradientBoostingClassifier(random_state=1),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(random_state=1),\n    SVC(probability=True)\n]\n\nresults = run_cross_validation(models, X_subset, y_subset, cv=5)\nprint(results)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''results = [\n    {'model_name': 'XGBClassifier', 'cv_scores': [0.61425, 0.6105, 0.61358333, 0.61016667, 0.61483333], 'mean_cv_score': 0.61267},\n    {'model_name': 'GaussianNB', 'cv_scores': [0.63325, 0.62608333, 0.63175, 0.63225, 0.63616667], 'mean_cv_score': 0.6319},\n    {'model_name': 'LogisticRegression', 'cv_scores': [0.63308333, 0.62625, 0.62925, 0.63233333, 0.635], 'mean_cv_score': 0.63118},\n    {'model_name': 'RandomForestClassifier', 'cv_scores': [0.61175, 0.61183333, 0.613, 0.60966667, 0.61608333], 'mean_cv_score': 0.61247},\n    {'model_name': 'GradientBoostingClassifier', 'cv_scores': [0.62625, 0.62375, 0.62791667, 0.62783333, 0.62983333], 'mean_cv_score': 0.62712},\n    {'model_name': 'KNeighborsClassifier', 'cv_scores': [0.57091667, 0.56625, 0.566, 0.56675, 0.56666667], 'mean_cv_score': 0.56732},\n    {'model_name': 'DecisionTreeClassifier', 'cv_scores': [0.53425, 0.54141667, 0.53958333, 0.53583333, 0.53825], 'mean_cv_score': 0.53787},\n    {'model_name': 'SVC', 'cv_scores': [0.6285, 0.624, 0.62466667, 0.63075, 0.63091667], 'mean_cv_score': 0.62777}\n]\n\ndf = pd.DataFrame(results)\nprint(\"We got this\")\nprint(df)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nxgb = XGBClassifier(random_state =1)\nlr = LogisticRegression(max_iter=2000)\ngnb = GaussianNB()\nrf = RandomForestClassifier(random_state=1)\nsvc = SVC(probability=True)\n\nvoting_clf = VotingClassifier(estimators = [('lr',lr),\n                                            ('gnb', gnb),\n                                            ('rf', rf),\n                                            ('svc',svc),\n                                            ('xgb',xgb)], voting = 'soft')\nprint(\"voting classifier:\")\n#cv = cross_val_score(voting_clf,X_subset,y_subset,cv=5)\n#print(cv)\n#print(cv.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"X_subset\")\nprint(X_subset)\nprint(\"X_test\")\nprint(X_test)\n#print(\"X_sig\")\n#print(X_sig)\nprint(\"X_train_sig\")\nprint(X_train_sig)\nprint(\"X_train_sig\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_clf.fit(X_backup,y_backup)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat_base_vc = voting_clf.predict(X_test_scaled).astype(float)\nsubmission = {'id': test.id , 'target': y_hat_base_vc}\nsubmission = pd.DataFrame(data=submission)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}