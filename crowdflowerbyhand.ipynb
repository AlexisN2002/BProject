{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-26T15:02:11.767563Z","iopub.execute_input":"2023-11-26T15:02:11.768006Z","iopub.status.idle":"2023-11-26T15:02:11.779443Z","shell.execute_reply.started":"2023-11-26T15:02:11.767974Z","shell.execute_reply":"2023-11-26T15:02:11.777225Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\nprint(training)\n\nfrom bs4 import BeautifulSoup\n'''def cleanHTML(phrase):\n    soup = BeautifulSoup(phrase,'lxml')\n    text = soup.get_text()\n    return text'''\n\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\n\ntraining['query'] = training['query'].apply(remove_html_tags)\ntest['query'] = test['query'].apply(remove_html_tags)\n\nstay_training = training\n#test = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\n#test['relevance_variance'] = np.NaN\n#training['train_test'] = 1\n#### I cant fill missing product desc with the average like I usually do\ntraining.product_description.fillna('',inplace=True)\ntest.product_description.fillna('',inplace=True)\n###\n#training = training.drop(['product_description'], axis=1)\n#test['train_test'] = 0\n#all_data = pd.concat([training,test]) \n\n\ntraining['query_product_title_words_overlap'] = np.zeros(training.shape[0])\ntest['query_product_title_words_overlap'] = np.zeros(test.shape[0])\n\n# Iterate through each row and count the overlapping words\nfor index, row in training.iterrows():\n    query_text = row['query'].lower().split()\n    product_title = row['product_title'].lower().split()\n    count_overlap = sum(word in product_title for word in query_text)\n    training.at[index, 'query_product_title_words_overlap'] = count_overlap\n\n# Visualize the distribution\nsns.countplot(data=training, x='query_product_title_words_overlap', hue='median_relevance')\nplt.title('Count of overlapping words vs. Target Column')\nplt.show()\n\n\n#print(training.head())\n#print(test.head())\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \n'''X_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)'''\n\ntraining['query'] = training.apply(lambda x : x['query'].replace(' ','_'),axis = 1)\ntest['query'] = test.apply(lambda x : x['query'].replace(' ','_'),axis = 1)\n\ntraining['product_title'] = training.apply(lambda x : x['product_title'].replace(' ','_'),axis = 1)\ntest['product_title'] = test.apply(lambda x : x['product_title'].replace(' ','_'),axis = 1)\n\ntraining['product_description'] = training.apply(lambda x : x['product_description'].replace(' ','_'),axis = 1)\ntest['product_description'] = test.apply(lambda x : x['product_description'].replace(' ','_'),axis = 1)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n'''vectorizer = CountVectorizer()\nvectorizer.fit(training['query'])\nquery = vectorizer.fit_transform(training['query'])\ntraining['query'] = query\n#query_cv = vectorizer.transform(df_cv['query'])\nquery = vectorizer.transform(test['query'])\ntest['query'] = query'''\nvectorizer = CountVectorizer()\nX_train_q = vectorizer.fit_transform(training['query'])\ntraining_transformed = pd.DataFrame(X_train_q.toarray(), columns=vectorizer.get_feature_names_out())\ntraining = pd.concat([training, training_transformed], axis=1)\nX_test_q = vectorizer.transform(test['query'])\ntest_transformed = pd.DataFrame(X_test_q.toarray(), columns=vectorizer.get_feature_names_out())\ntest = pd.concat([test, test_transformed], axis=1)\nprint('##################')\nprint(training)\n\n'''tfidf_product_title_vectorizer = TfidfVectorizer(min_df=3,max_df=300)\nproduct_title = tfidf_product_title_vectorizer.fit_transform(training['product_title'])\ntraining['product_title'] = product_title\n#product_title_tfidf_cv = tfidf_product_title_vectorizer.transform(df_cv['ste_cleaned_product_title'])\nproduct_title = tfidf_product_title_vectorizer.transform(test['product_title'])\ntest['product_title'] = product_title'''\ntfidf_product_title_vectorizer = TfidfVectorizer(min_df=3, max_df=300)\nX_train_product_title = tfidf_product_title_vectorizer.fit_transform(training['product_title'])\ntraining_transformed_product_title = pd.DataFrame(X_train_product_title.toarray(), columns=tfidf_product_title_vectorizer.get_feature_names_out())\ntraining = pd.concat([training, training_transformed_product_title], axis=1)\nX_test_product_title = tfidf_product_title_vectorizer.transform(test['product_title'])\ntest_transformed_product_title = pd.DataFrame(X_test_product_title.toarray(), columns=tfidf_product_title_vectorizer.get_feature_names_out())\ntest = pd.concat([test, test_transformed_product_title], axis=1)\n\ntfidf_product_title_vectorizer = TfidfVectorizer(min_df=3, max_df=300)\nX_train_product_title = tfidf_product_title_vectorizer.fit_transform(training['product_description'])\ntraining_transformed_product_title = pd.DataFrame(X_train_product_title.toarray(), columns=tfidf_product_title_vectorizer.get_feature_names_out())\ntraining = pd.concat([training, training_transformed_product_title], axis=1)\nX_test_product_title = tfidf_product_title_vectorizer.transform(test['product_description'])\ntest_transformed_product_title = pd.DataFrame(X_test_product_title.toarray(), columns=tfidf_product_title_vectorizer.get_feature_names_out())\ntest = pd.concat([test, test_transformed_product_title], axis=1)\nprint('####training####')\nprint(training)\n\ny_train = training['median_relevance']\nX_train = training.drop(['query', 'product_description','product_title'], axis=1)\nX_train = X_train.drop(['median_relevance'], axis=1)\nX_test = test\n#y_train = training['median_relevance']\n#X_test = test.drop(['median_relevance'], axis=1)\nX_backup = X_train\n#X_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\n#y_train = all_data[all_data.train_test==1].median_relevance\ny_check = y_train\ny_backup = y_train\n#X_test = X_test.drop(['median_relevance'], axis=1)\n'''for column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)'''\nprint('###########X_train################')\nprint(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T15:02:11.787146Z","iopub.execute_input":"2023-11-26T15:02:11.787601Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"          id                       query  \\\n0          1   bridal shower decorations   \n1          2        led christmas lights   \n2          4                   projector   \n3          5                   wine rack   \n4          7                  light bulb   \n...      ...                         ...   \n10153  32655  plantronics corded headset   \n10154  32659                   spiderman   \n10155  32663     playstation vita system   \n10156  32666                flower bulbs   \n10157  32668    polar heart rate monitor   \n\n                                           product_title  \\\n0            Accent Pillow with Heart Design - Red/Black   \n1      Set of 10 Battery Operated Multi LED Train Chr...   \n2             ViewSonic Pro8200 DLP Multimedia Projector   \n3      Concept Housewares WR-44526 Solid-Wood Ceiling...   \n4      Wintergreen Lighting Christmas LED Light Bulb ...   \n...                                                  ...   \n10153                 Plantronics TriStar H81N - headset   \n10154  Marvel Legends Infinite Series Amazing Spider-...   \n10155                MLB 15: The Show (PlayStation Vita)   \n10156  Junkyard Findings Vintage Trinkets-Small Typo ...   \n10157  Polar F6 Black Coal Heart Rate Monitor Mens Pi...   \n\n                                     product_description  median_relevance  \\\n0      Red satin accent pillow embroidered with a hea...                 1   \n1      Set of 10 Battery Operated Train Christmas Lig...                 4   \n2                                                    NaN                 4   \n3      Like a silent and sturdy tree, the Southern En...                 4   \n4      WTGR1011\\nFeatures\\nNickel base, 60,000 averag...                 2   \n...                                                  ...               ...   \n10153  With four sizes of user-selectable earbuds and...                 4   \n10154                                                NaN                 2   \n10155  An essential part of every fanâ€™s season and Th...                 2   \n10156  PRIMA FLOWERS-Junkyard Findings Vintage Trinke...                 1   \n10157                                                NaN                 4   \n\n       relevance_variance  \n0                   0.000  \n1                   0.000  \n2                   0.471  \n3                   0.000  \n4                   0.471  \n...                   ...  \n10153               0.471  \n10154               0.471  \n10155               1.247  \n10156               0.471  \n10157               0.000  \n\n[10158 rows x 6 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Feature Engineering","metadata":{}},{"cell_type":"code","source":"x = stay_training.median_relevance.value_counts().keys()\ny = stay_training.median_relevance.value_counts()\n\n\nax = stay_training.median_relevance.value_counts().plot(kind='bar',color=['C0', 'C1', 'C2','C3'])\nfor i,j in zip(x,y):\n    plt.annotate(str(round(j/stay_training.shape[0]*100,2))+'%', xy=(i,j), ha='center', va='bottom')\nplt.title('Bar plot of the target values')\nplt.xlabel('Target values')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = stay_training['query'].value_counts().tolist()\ny  = sorted(y,reverse=True)\nx = range(len(y))\nplt.bar(x,y)\nplt.ylabel('Count')\nplt.xlabel('Category')\nplt.title('Bar plot of the query text categories and the count')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the data is very skewed, most queries appear only once and a lot of search results seem accurate.\n\nI noticed in the data that some queries are essentially the same but differ in a blank space or a dash so I will put those together using editing distance. I excluded numbers causing this editing distance such that different \"versions\" of a product remain sepperate like playstation 3 and 4.\nUnfortunately this was not possible right now due to long computation time.","metadata":{}},{"cell_type":"code","source":"!pip install editdistance\nimport editdistance\n\n'''def merge_similar_attributes(X_train):\n    merged_df = X_train.copy()\n    merged_df = merged_df.drop(['query_product_title_words_overlap'], axis=1)\n    attribute_names = X_train.columns.tolist()\n    merged_attributes = {}\n    for i, attr1 in enumerate(attribute_names):\n        for j, attr2 in enumerate(attribute_names[i+1:]):\n            distance = editdistance.eval(attr1, attr2)\n            if distance == 1 and not any(c.isdigit() for c in set(attr1).symmetric_difference(attr2)):\n                merged_name = f\"{attr1}_{attr2}\"\n                merged_df[merged_name] = X_train[attr1].astype(str) + X_train[attr2].astype(str)\n                merged_attributes[merged_name] = (attr1, attr2)\n\n    return merged_df, merged_attributes'''\n\ndef merge_similar_attributes(X_train):\n    merged_df = X_train.copy()\n    merged_df = merged_df.drop(['query_product_title_words_overlap'], axis=1)\n    attribute_names = X_train.columns.tolist()\n    merged_attributes = {}\n\n    for i, attr1 in enumerate(attribute_names):\n        for j, attr2 in enumerate(attribute_names[i+1:]):\n            distance = editdistance.eval(str(attr1), str(attr2))\n            if distance == 1 and not any(c.isdigit() for c in set(str(attr1)).symmetric_difference(str(attr2))):\n                merged_name = f\"{attr1}_{attr2}\"\n                merged_df[merged_name] = X_train[attr1].astype(str) + X_train[attr2].astype(str)\n                merged_attributes[merged_name] = (attr1, attr2)\n\n    return merged_df, merged_attributes\n\nX_train, merged_attributes_dict = merge_similar_attributes(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My next thought was that the same word appearing in both querry and product should have an impact","metadata":{}},{"cell_type":"code","source":"stay_training['query_product_title_words_overlap'] = np.zeros(stay_training.shape[0])\nfor index, query_text in enumerate(stay_training['query']):\n    count = 0\n    query_text = query_text.lower().split()\n    product_title = stay_training.product_title[index].lower() \n    for word in query_text:\n        if word in product_title:\n            count+=1\n\n    stay_training.loc[index,'query_product_title_words_overlap'] = count\n    \nsns.countplot(data = stay_training, x = 'query_product_title_words_overlap', hue='median_relevance')\nplt.title('Count of words from query that are in  product title')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This turned out to be true so Im adding a feature to reflect this (above due to X_train already being edited)","metadata":{}},{"cell_type":"code","source":"print(training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntarget_variable = 'median_relevance'\nnumeric_data = training.select_dtypes(include='number')\ncorrelation_matrix = numeric_data.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix[[target_variable]], annot=True, cmap='coolwarm', linewidths=.5)\n\nplt.title(f'Correlation Heatmap with {target_variable}')\nplt.show()\n\n#due to the lack of numerical values, these types of visualization provide little information","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''target_variable = 'relevance_variance'\ncorrelations = X_train.corr()[target_column]\ncorrelations = correlations.drop(target_variable)\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.1) | (correlations <= -0.1)]\nsignificant_features_index = correlations[(correlations >= 0.1) | (correlations <= -0.1)].index\nprint(significant_features)\nprint(significant_features_index)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_train = pd.concat([X_train, y_train], axis=1)\n\ndef create_splits(X_train, y_train, num_splits):\n    total_features = X_train.shape[1]\n    features_per_subset = total_features // num_splits\n    all_train_splits = []\n    for i in range(num_splits):\n        start_idx = i * features_per_subset\n        end_idx = (i + 1) * features_per_subset if i < num_splits - 1 else total_features\n        subset_features = X_train.iloc[:, start_idx:end_idx]\n        all_train_subset = pd.concat([subset_features, y_train], axis=1)\n        all_train_splits.append(all_train_subset)\n\n    return all_train_splits\n\nnum_splits = 3500\nall_train_splits = create_splits(X_train, y_train, num_splits)\n\n\n#print(all_train_splits[0].head())\nplt.figure(figsize=(16, 8))\ncorrelations = {}\n\nfrom scipy.stats import pearsonr\n\ndef feature_stats(dataset, targ, correlation_threshold=0.05):\n    features = dataset.drop(columns=[targ])\n    target = dataset[targ]\n    correlation_results = [pearsonr(features[feature], target) for feature in features.columns]\n    selected_positive = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr >= correlation_threshold]\n    selected_negative = [(feature, corr, p) for (feature, (corr, p)) in zip(features.columns, correlation_results) if corr <= -correlation_threshold]\n\n    # Create a DataFrame to store the results\n    stats_df = pd.DataFrame({\n        'Feature': [item[0] for item in selected_positive + selected_negative],\n        'Correlation': [item[1] for item in selected_positive + selected_negative],\n        'P-Value': [item[2] for item in selected_positive + selected_negative],\n    })\n\n    return stats_df\n\nstats_all_train_splits_0 = feature_stats(all_train_splits[0], 'median_relevance')\nprint('###############')\nprint(stats_all_train_splits_0)\nprint('###############')\n\n\ndef plot_feature_heatmap(dataset, targ, correlation_threshold=0.05):\n    # Extract features and target\n    features = dataset.drop(columns=[targ])  \n    target = dataset[targ]\n    feature_correlation = features.corrwith(target)\n    selected_features = feature_correlation[(feature_correlation >= correlation_threshold) | (feature_correlation <= -correlation_threshold)]\n\n    # Create a DataFrame to store the results\n    corr_df = pd.DataFrame({'Correlation': selected_features})\n    feature_correlation_list = list(zip(selected_features.index, selected_features.values))\n    for feature, correlation in feature_correlation_list:\n        print(f\"{feature}: {correlation}\")\n    if not corr_df.empty:\n        plt.figure(figsize=(50, 20))\n        sns.heatmap(corr_df.transpose(), annot=True, cmap='coolwarm', linewidths=.5)\n        plt.title('Feature Correlation Heatmap')\n        plt.show()\n    #else:\n        #print(\"No features meet the correlation threshold criteria.\")\n\n    return feature_correlation_list\n\n#plot_feature_heatmap(all_train_splits[0], 'relevance_variance')\ndef plot_clustered_heatmap(dataset, targ):\n    features = dataset.drop(columns=[targ])  \n    target = dataset[targ]\n    feature_correlation = features.corrwith(target, method='spearman')\n    corr_df = pd.DataFrame({'Correlation': feature_correlation})\n    plt.figure(figsize=(20, 10))\n    sns.clustermap(features.corr(method='spearman'), cmap='coolwarm', annot=True, linewidths=.5)\n    plt.title('Clustered Feature Correlation Heatmap')\n    plt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3500):\n    #print('###############')\n    #here: \n    plot_feature_heatmap(all_train_splits[i], 'median_relevance')\n    #plot_clustered_heatmap(all_train_splits[i], 'median_relevance')\n    #stats_all_train_splits_0 = feature_stats(all_train_splits[i], 'relevance_variance')\n    #print(stats_all_train_splits_0)\n    #print('###############')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_significance(all_train_1, all_train_1['relevance_variance'], 'Split 1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attribute_names = training.columns.tolist()\nprint(attribute_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First attempt at Models","metadata":{}},{"cell_type":"code","source":"X_backup = X_train\nX_train_sig = X_train\nnum_samples = int(len(X_train) * 0.07)\nrandom_indices = np.random.choice(len(X_train), num_samples, replace=False)\nX_subset = X_train.iloc[random_indices]\ny_subset = y_train.iloc[random_indices]\n\nX_check = X_train.iloc[random_indices]\ny_check = y_train.iloc[random_indices]\n\nnum_samples = int(len(X_backup) * 0.07)\nrandom_indices = np.random.choice(len(X_backup), num_samples, replace=False)\nX_backup = X_backup.iloc[random_indices]\ny_backup = y_backup.iloc[random_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into training and temporary backup sets\nX_train_temp, X_backup, y_train_sig, y_backup = train_test_split(X_train_sig, y_train, test_size=0.2, random_state=1)\n\n# Split the backup set into a new validation set\nX_backup, X_val, y_backup, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n\n# Ensure that the indices are reset for each subset\nX_train_sig = X_train_temp.reset_index(drop=True)\nX_val = X_val.reset_index(drop=True)\nX_backup = X_backup.reset_index(drop=True)\ny_train = y_train_sig.reset_index(drop=True)\ny_val = y_val.reset_index(drop=True)\ny_backup = y_backup.reset_index(drop=True)\nnum_samples = int(len(X_train_sig) * 0.07)\nrandom_indices = np.random.choice(len(X_train_sig), num_samples, replace=False)\nX_subset = X_train_sig.iloc[random_indices]\ny_subset = y_train.iloc[random_indices]\n\nnum_samples = int(len(X_backup) * 0.07)\nrandom_indices = np.random.choice(len(X_backup), num_samples, replace=False)\nX_backup = X_backup.iloc[random_indices]\ny_backup = y_backup.iloc[random_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\n\n# Get the current date and time\ncurrent_time = datetime.datetime.now()\n\n# Convert the current time to a string with a specific format\nformatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n# Create a string with the formatted time\ntime = f\"{formatted_time}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import  accuracy_score, cohen_kappa_score, make_scorer\nkappa_scorer = make_scorer(cohen_kappa_score,weights = 'quadratic', greater_is_better = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nwandb.init(project='CrowdflowerByHand', name='untuned attemps '+time)\n\ndef run_cross_validation(models, X_train, y_train, cv=5):\n    results = []\n\n    for model in models:\n        model_name = model.__class__.__name__\n\n        '''if model_name == 'XGBClassifier':\n            class_counts = y_train.value_counts()\n            class_weight = class_counts[0] / class_counts[1]\n            model = XGBClassifier(scale_pos_weight=class_weight, random_state=1)'''\n        \n        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)\n        \n        # Log the results to WandB:\n        wandb.log({f'{model_name}_CV_Scores': cv_scores.tolist(), f'{model_name}_Mean_CV_Score': np.mean(cv_scores)})\n        \n        results.append({\n            'model_name': model_name,\n            'cv_scores': cv_scores,\n            'mean_cv_score': np.mean(cv_scores),\n            #'parameters': {\n                #'scale_pos_weight': class_weight if model_name == 'XGBClassifier' else None\n            #}\n        })\n\n    return results\n\nxgb = XGBClassifier(random_state =1)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n#model = []\nmodels = [\n    xgb,\n    LinearSVC(),\n    GaussianNB(),\n    LogisticRegression(max_iter=2000),\n    RandomForestClassifier(random_state=1),\n    GradientBoostingClassifier(random_state=1),\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(random_state=1),\n    SVC(probability=True)\n]\nmodels_2 = []\nfor mod in models:\n    mod_2 = GridSearchCV(mod, param_grid={},scoring=kappa_scorer)\n    models_2.append(mod_2)\n    \n#print(models_2)\n#results = run_cross_validation(models_2, X_subset, y_subset, cv=5)\n#print(results)\n'''svc = LinearSVC()\nsvc = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},scoring=kappa_scorer)\ncv = cross_val_score(svc,X_subset,y_subset,cv=5)'''\nparam_grid = {'svd__n_components' : [200, 400],\n              'svm__C': [10, 12]}\n\nsvc = GridSearchCV(estimator = SVC(), param_grid={}, scoring=kappa_scorer,\n                                  verbose=True, n_jobs=-1, refit=True, cv=5)\nsvc.fit(X_check, y_check)\nprint(\"Best score: %0.3f\" % svc.best_score_)\n#cv = cross_val_score(svc,X_subset,y_subset,cv=5)\n#print(cv)\n#print(cv.mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}