{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport wandb\nfrom wandb.keras import WandbCallback\nos.system('! wandb login be213aaff4ff14945d480abc18697d8664bba8c8')\ntraining = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-nov-2021/test.csv')\ntest['target'] = np.NaN\ntraining['train_test'] = 1\ntest['train_test'] = 0\nall_data = pd.concat([training,test]) \n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nattribute_names = all_data.columns.tolist()\ntest_atts = test.columns.tolist()\nall_dummies = pd.get_dummies(all_data[attribute_names])\ntest_dummies = pd.get_dummies(test[test_atts])  \nX_train = all_dummies[all_data.train_test == 1].drop(['train_test'], axis=1)\nfor column in X_train.columns:\n    mean_value = X_train[column].mean()\n    X_train[column].fillna(mean_value, inplace=True)\nX_check = X_train #for heatmap\nX_train = X_train.drop(['target'], axis=1)\nX_backup = X_train\nX_test = all_dummies[all_data.train_test == 0].drop(['train_test'], axis=1)\ny_train = all_data[all_data.train_test==1].target\ny_backup = y_train\nX_test = X_test.drop(['target'], axis=1)\nfor column in X_test.columns:\n    mean_value = X_test[column].mean()\n    X_test[column].fillna(mean_value, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nX_adapt = X_train.drop(['id'], axis=1)\nall_attributes =X_adapt.columns.tolist()\ncolumns_to_normalize = all_attributes\nscaler = MinMaxScaler()\nX_train_scaled = X_train\nX_train_scaled[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n#X_train_scaled = X_train_scaled[:,~np.all(np.isnan(d), axis=0)]\nX_test_scaled = X_test\nX_test_scaled[columns_to_normalize] = scaler.fit_transform(X_test[columns_to_normalize])\n\n#also norm. backup\nX_backup[columns_to_normalize] = scaler.fit_transform(X_backup[columns_to_normalize])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_column = 'target'\n\n# Calculate the correlations between the target and all features\ncorrelations = training.corr()[target_column]\ncorrelations = correlations.drop('target')\n# Filter features with a correlation of at least 0.4 or -0.4\nsignificant_features = correlations[(correlations >= 0.1) | (correlations <= -0.1)]\nsignificant_features_index = correlations[(correlations >= 0.1) | (correlations <= -0.1)].index\n\n# Visualize the correlation values\nplt.figure(figsize=(10, 6))\nsns.barplot(x=significant_features.values, y=significant_features.index)\nplt.title(f'Features Correlating at least 0.1 or -0.1 with {target_column}')\nplt.xlabel('Correlation')\nplt.ylabel('Features')\nplt.show()\nX_train_sig = X_train_scaled[significant_features_index]\nprint(X_train_sig)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.init(project='KaggleNovemer2021NN', name='new approach with NN')\n#X = data.drop('target', axis=1)  # Features\n#y = data['target']  # Labels (binary classification)\n\n# Split the data into train and test sets\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train = torch.tensor(X_train_sig.values, dtype=torch.float32)\ny_train = torch.tensor(y_train.values, dtype=torch.int64)\nX_test = torch.tensor(X_test.values, dtype=torch.float32)\ny_test = torch.tensor(y_test.values, dtype=torch.int64)\n\n# Define a simple neural network model\nclass SimpleClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleClassifier, self).__init()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Initialize the model\ninput_size = X_train.shape[1]\nhidden_size = 64\noutput_size = 2  # Change this to match your number of classes\nmodel = SimpleClassifier(input_size, hidden_size, output_size)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    # Log the loss and other metrics to WandB\n    wandb.log({\"train_loss\": loss.item()}, step=epoch)\n\n# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_test)\n    _, predicted = torch.max(y_pred, 1)\n    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n\nwandb.log({\"test_accuracy\": accuracy})\n\n# Save the model\ntorch.save(model.state_dict(), \"model.pth\")\n\n# Finish logging\nwandb.finish()","metadata":{},"execution_count":null,"outputs":[]}]}