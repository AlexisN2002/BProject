{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4407,"databundleVersionId":34172,"sourceType":"competition"}],"dockerImageVersionId":30357,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T21:01:10.740539Z","iopub.execute_input":"2023-11-29T21:01:10.741420Z","iopub.status.idle":"2023-11-29T21:01:10.772826Z","shell.execute_reply.started":"2023-11-29T21:01:10.741386Z","shell.execute_reply":"2023-11-29T21:01:10.771628Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"/kaggle/input/crowdflower-search-relevance/train.csv.zip\n/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip\n/kaggle/input/crowdflower-search-relevance/test.csv.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/crowdflower-search-relevance/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/crowdflower-search-relevance/test.csv.zip')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:10.774617Z","iopub.execute_input":"2023-11-29T21:01:10.775166Z","iopub.status.idle":"2023-11-29T21:01:11.104641Z","shell.execute_reply.started":"2023-11-29T21:01:10.775132Z","shell.execute_reply":"2023-11-29T21:01:11.103262Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   id                      query  \\\n0   1  bridal shower decorations   \n1   2       led christmas lights   \n2   4                  projector   \n3   5                  wine rack   \n4   7                 light bulb   \n\n                                       product_title  \\\n0        Accent Pillow with Heart Design - Red/Black   \n1  Set of 10 Battery Operated Multi LED Train Chr...   \n2         ViewSonic Pro8200 DLP Multimedia Projector   \n3  Concept Housewares WR-44526 Solid-Wood Ceiling...   \n4  Wintergreen Lighting Christmas LED Light Bulb ...   \n\n                                 product_description  median_relevance  \\\n0  Red satin accent pillow embroidered with a hea...                 1   \n1  Set of 10 Battery Operated Train Christmas Lig...                 4   \n2                                                NaN                 4   \n3  Like a silent and sturdy tree, the Southern En...                 4   \n4  WTGR1011\\nFeatures\\nNickel base, 60,000 averag...                 2   \n\n   relevance_variance  \n0               0.000  \n1               0.000  \n2               0.471  \n3               0.000  \n4               0.471  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>query</th>\n      <th>product_title</th>\n      <th>product_description</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>bridal shower decorations</td>\n      <td>Accent Pillow with Heart Design - Red/Black</td>\n      <td>Red satin accent pillow embroidered with a hea...</td>\n      <td>1</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>led christmas lights</td>\n      <td>Set of 10 Battery Operated Multi LED Train Chr...</td>\n      <td>Set of 10 Battery Operated Train Christmas Lig...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>projector</td>\n      <td>ViewSonic Pro8200 DLP Multimedia Projector</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>0.471</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>wine rack</td>\n      <td>Concept Housewares WR-44526 Solid-Wood Ceiling...</td>\n      <td>Like a silent and sturdy tree, the Southern En...</td>\n      <td>4</td>\n      <td>0.000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>light bulb</td>\n      <td>Wintergreen Lighting Christmas LED Light Bulb ...</td>\n      <td>WTGR1011\\nFeatures\\nNickel base, 60,000 averag...</td>\n      <td>2</td>\n      <td>0.471</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef remove_html_tags(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\n\ntrain['query'] = train['query'].apply(remove_html_tags)\ntest['query'] = test['query'].apply(remove_html_tags)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:11.106260Z","iopub.execute_input":"2023-11-29T21:01:11.106600Z","iopub.status.idle":"2023-11-29T21:01:12.905864Z","shell.execute_reply.started":"2023-11-29T21:01:11.106570Z","shell.execute_reply":"2023-11-29T21:01:12.904808Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:12.908078Z","iopub.execute_input":"2023-11-29T21:01:12.909085Z","iopub.status.idle":"2023-11-29T21:01:12.927429Z","shell.execute_reply.started":"2023-11-29T21:01:12.909053Z","shell.execute_reply":"2023-11-29T21:01:12.925723Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10158 entries, 0 to 10157\nData columns (total 6 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   id                   10158 non-null  int64  \n 1   query                10158 non-null  object \n 2   product_title        10158 non-null  object \n 3   product_description  7714 non-null   object \n 4   median_relevance     10158 non-null  int64  \n 5   relevance_variance   10158 non-null  float64\ndtypes: float64(1), int64(2), object(3)\nmemory usage: 476.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:12.928959Z","iopub.execute_input":"2023-11-29T21:01:12.929394Z","iopub.status.idle":"2023-11-29T21:01:12.949369Z","shell.execute_reply.started":"2023-11-29T21:01:12.929348Z","shell.execute_reply":"2023-11-29T21:01:12.948556Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 22513 entries, 0 to 22512\nData columns (total 4 columns):\n #   Column               Non-Null Count  Dtype \n---  ------               --------------  ----- \n 0   id                   22513 non-null  int64 \n 1   query                22513 non-null  object\n 2   product_title        22513 non-null  object\n 3   product_description  17086 non-null  object\ndtypes: int64(1), object(3)\nmemory usage: 703.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:12.950301Z","iopub.execute_input":"2023-11-29T21:01:12.950616Z","iopub.status.idle":"2023-11-29T21:01:12.971857Z","shell.execute_reply.started":"2023-11-29T21:01:12.950590Z","shell.execute_reply":"2023-11-29T21:01:12.970543Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount  10158.000000      10158.000000        10158.000000\nmean   16353.103071          3.309805            0.377863\nstd     9447.106683          0.980666            0.389707\nmin        1.000000          1.000000            0.000000\n25%     8078.750000          3.000000            0.000000\n50%    16349.500000          4.000000            0.471000\n75%    24570.750000          4.000000            0.471000\nmax    32668.000000          4.000000            1.470000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n      <td>10158.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>16353.103071</td>\n      <td>3.309805</td>\n      <td>0.377863</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>9447.106683</td>\n      <td>0.980666</td>\n      <td>0.389707</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8078.750000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16349.500000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>24570.750000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32668.000000</td>\n      <td>4.000000</td>\n      <td>1.470000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['query'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:12.973594Z","iopub.execute_input":"2023-11-29T21:01:12.974330Z","iopub.status.idle":"2023-11-29T21:01:12.990938Z","shell.execute_reply.started":"2023-11-29T21:01:12.974268Z","shell.execute_reply":"2023-11-29T21:01:12.989353Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"2    5379\n3    2819\n4     925\n1     885\n6      81\n5      69\nName: query, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train['product_title'].map(lambda x:len(x.split())).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:12.992405Z","iopub.execute_input":"2023-11-29T21:01:12.992748Z","iopub.status.idle":"2023-11-29T21:01:13.013859Z","shell.execute_reply.started":"2023-11-29T21:01:12.992718Z","shell.execute_reply":"2023-11-29T21:01:13.012016Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"7     1288\n6     1284\n8     1183\n9     1122\n5     1002\n10     880\n11     744\n12     678\n4      550\n13     453\n14     289\n15     181\n3      174\n17      78\n16      66\n2       51\n18      25\n19      24\n20      15\n24      11\n21       7\n25       7\n27       6\n26       6\n22       6\n28       6\n23       6\n29       4\n1        2\n46       2\n32       2\n38       1\n44       1\n34       1\n31       1\n43       1\n41       1\nName: product_title, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"Feature engineering before vector","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction import text\nimport string\n\n# Initialize the Porter Stemmer\nstemmer = PorterStemmer()\n\n# Stopwords and Punctuation\nstop_words = ['http', 'www', 'img', 'border', 'color', 'style', 'padding', 'table', 'font', 'thi', 'inch', 'ha', 'width', 'height',\n              '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nstop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n\npunct = string.punctuation\npunct_re = re.compile('[{}]'.format(re.escape(punct)))\n\ncolumns_to_preprocess = ['query', 'product_title', 'product_description']\n\ndef preprocess_text_column(df, column):\n    df[column] = df[column].apply(lambda x: preprocess(str(x)))\n    return df\n\ndef preprocess(x):\n    x = x.lower()\n    x = punct_re.sub(' ', x)\n    new_x = []\n    for token in x.split(' '):\n        if token not in stop_words:\n            new_x.append(stemmer.stem(token))\n    return ' '.join(new_x)\n\n# Example usage:\n# Assuming you have 'train' and 'test' DataFrames\ntrain = preprocess_text_column(train, 'query')\ntrain = preprocess_text_column(train, 'product_title')\ntrain = preprocess_text_column(train, 'product_description')\n\ntest = preprocess_text_column(test, 'query')\ntest = preprocess_text_column(test, 'product_title')\ntest = preprocess_text_column(test, 'product_description')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:13.019948Z","iopub.execute_input":"2023-11-29T21:01:13.020330Z","iopub.status.idle":"2023-11-29T21:01:58.402077Z","shell.execute_reply.started":"2023-11-29T21:01:13.020263Z","shell.execute_reply":"2023-11-29T21:01:58.400877Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"split = int(len(train)*0.8)\ntrain_0, dev = train[:split], train[split:]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.403237Z","iopub.execute_input":"2023-11-29T21:01:58.403648Z","iopub.status.idle":"2023-11-29T21:01:58.411627Z","shell.execute_reply.started":"2023-11-29T21:01:58.403622Z","shell.execute_reply":"2023-11-29T21:01:58.410262Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"clean_train_1 = train_0[train_0.relevance_variance <1].copy()\nclean_train_2 = train_0[train_0.relevance_variance <0.50].copy()\ndev.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.413086Z","iopub.execute_input":"2023-11-29T21:01:58.413527Z","iopub.status.idle":"2023-11-29T21:01:58.443687Z","shell.execute_reply.started":"2023-11-29T21:01:58.413490Z","shell.execute_reply":"2023-11-29T21:01:58.442387Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   2032.000000       2032.000000         2032.000000\nmean   29406.014764          3.319390            0.361364\nstd     1870.217123          0.972218            0.379619\nmin    26215.000000          1.000000            0.000000\n25%    27777.250000          3.000000            0.000000\n50%    29410.000000          4.000000            0.471000\n75%    31014.750000          4.000000            0.471000\nmax    32668.000000          4.000000            1.470000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2032.000000</td>\n      <td>2032.000000</td>\n      <td>2032.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>29406.014764</td>\n      <td>3.319390</td>\n      <td>0.361364</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1870.217123</td>\n      <td>0.972218</td>\n      <td>0.379619</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>26215.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>27777.250000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>29410.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>31014.750000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32668.000000</td>\n      <td>4.000000</td>\n      <td>1.470000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = clean_train_1\ntrain_input = train.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ndev_input =  dev.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.444855Z","iopub.execute_input":"2023-11-29T21:01:58.445158Z","iopub.status.idle":"2023-11-29T21:01:58.883730Z","shell.execute_reply.started":"2023-11-29T21:01:58.445132Z","shell.execute_reply":"2023-11-29T21:01:58.882606Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"clean_train_1.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.884968Z","iopub.execute_input":"2023-11-29T21:01:58.885299Z","iopub.status.idle":"2023-11-29T21:01:58.908614Z","shell.execute_reply.started":"2023-11-29T21:01:58.885255Z","shell.execute_reply":"2023-11-29T21:01:58.907338Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   7558.000000       7558.000000         7558.000000\nmean   13074.201111          3.344403            0.321038\nstd     7571.543134          0.974908            0.332482\nmin        1.000000          1.000000            0.000000\n25%     6496.500000          3.000000            0.000000\n50%    13129.000000          4.000000            0.471000\n75%    19563.250000          4.000000            0.471000\nmax    26208.000000          4.000000            0.980000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7558.000000</td>\n      <td>7558.000000</td>\n      <td>7558.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13074.201111</td>\n      <td>3.344403</td>\n      <td>0.321038</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7571.543134</td>\n      <td>0.974908</td>\n      <td>0.332482</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6496.500000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13129.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>19563.250000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>26208.000000</td>\n      <td>4.000000</td>\n      <td>0.980000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"clean_train_2.describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.909964Z","iopub.execute_input":"2023-11-29T21:01:58.910301Z","iopub.status.idle":"2023-11-29T21:01:58.935035Z","shell.execute_reply.started":"2023-11-29T21:01:58.910257Z","shell.execute_reply":"2023-11-29T21:01:58.933326Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                 id  median_relevance  relevance_variance\ncount   6206.000000       6206.000000         6206.000000\nmean   13154.066549          3.432646            0.202590\nstd     7570.559783          0.959901            0.232434\nmin        1.000000          1.000000            0.000000\n25%     6576.750000          3.000000            0.000000\n50%    13299.500000          4.000000            0.000000\n75%    19708.000000          4.000000            0.471000\nmax    26208.000000          4.000000            0.490000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>median_relevance</th>\n      <th>relevance_variance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>6206.000000</td>\n      <td>6206.000000</td>\n      <td>6206.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>13154.066549</td>\n      <td>3.432646</td>\n      <td>0.202590</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7570.559783</td>\n      <td>0.959901</td>\n      <td>0.232434</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6576.750000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>13299.500000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>19708.000000</td>\n      <td>4.000000</td>\n      <td>0.471000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>26208.000000</td>\n      <td>4.000000</td>\n      <td>0.490000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer \ntfidf = TfidfVectorizer(ngram_range=(1, 5),stop_words = 'english', strip_accents='unicode')\ntrain_x = tfidf.fit_transform(train_input)\ndev_x = tfidf.transform(dev_input)\ntest_x = tfidf.transform(test_input)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:01:58.936723Z","iopub.execute_input":"2023-11-29T21:01:58.937128Z","iopub.status.idle":"2023-11-29T21:02:00.791519Z","shell.execute_reply.started":"2023-11-29T21:01:58.937088Z","shell.execute_reply":"2023-11-29T21:02:00.790291Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(train_x)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:00.792938Z","iopub.execute_input":"2023-11-29T21:02:00.793207Z","iopub.status.idle":"2023-11-29T21:02:00.799625Z","shell.execute_reply.started":"2023-11-29T21:02:00.793182Z","shell.execute_reply":"2023-11-29T21:02:00.798613Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"  (0, 109450)\t0.19624310126460465\n  (0, 9600)\t0.19624310126460465\n  (0, 45196)\t0.19624310126460465\n  (0, 131476)\t0.19624310126460465\n  (0, 25104)\t0.17677693475696124\n  (0, 70893)\t0.19624310126460465\n  (0, 109449)\t0.19624310126460465\n  (0, 9599)\t0.19624310126460465\n  (0, 45195)\t0.19624310126460465\n  (0, 131474)\t0.17677693475696124\n  (0, 25103)\t0.17677693475696124\n  (0, 46173)\t0.19624310126460465\n  (0, 70892)\t0.19624310126460465\n  (0, 109448)\t0.19624310126460465\n  (0, 9598)\t0.19624310126460465\n  (0, 45193)\t0.17677693475696124\n  (0, 131473)\t0.17677693475696124\n  (0, 25100)\t0.13943206840666683\n  (0, 119917)\t0.16205134825726433\n  (0, 46172)\t0.19624310126460465\n  (0, 70888)\t0.18151751476490768\n  (0, 109447)\t0.19624310126460465\n  (0, 9596)\t0.17677693475696124\n  (0, 45192)\t0.17677693475696124\n  (0, 131469)\t0.13943206840666683\n  :\t:\n  (7557, 58277)\t0.1840052250601992\n  (7557, 35956)\t0.1759284798179473\n  (7557, 89323)\t0.1759284798179473\n  (7557, 108549)\t0.1701979378712004\n  (7557, 108548)\t0.1701979378712004\n  (7557, 155859)\t0.15194569702188462\n  (7557, 36103)\t0.15905055659889938\n  (7557, 89637)\t0.15905055659889938\n  (7557, 68461)\t0.15905055659889938\n  (7557, 17312)\t0.15905055659889938\n  (7557, 6340)\t0.1657529842108834\n  (7557, 58260)\t0.15639065068220168\n  (7557, 36100)\t0.14258336349320294\n  (7557, 36099)\t0.14258336349320294\n  (7557, 89630)\t0.13535439320468984\n  (7557, 89629)\t0.13535439320468984\n  (7557, 108534)\t0.12642987300869918\n  (7557, 108533)\t0.12642987300869918\n  (7557, 108532)\t0.12642987300869918\n  (7557, 68421)\t0.12243257200197248\n  (7557, 17244)\t0.11154558273503552\n  (7557, 68420)\t0.12243257200197248\n  (7557, 35941)\t0.19293819067039328\n  (7557, 35611)\t0.16747079187285138\n  (7557, 89302)\t0.1756532818905111\n","output_type":"stream"}]},{"cell_type":"markdown","source":"adding further feature engineering","metadata":{}},{"cell_type":"code","source":"print(train_input)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:00.800850Z","iopub.execute_input":"2023-11-29T21:02:00.801219Z","iopub.status.idle":"2023-11-29T21:02:00.813471Z","shell.execute_reply.started":"2023-11-29T21:02:00.801186Z","shell.execute_reply":"2023-11-29T21:02:00.811593Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"0       bridal shower decor accent pillow heart design...\n1       led christma light set 10 batteri oper multi l...\n2       projector viewson pro8200 dlp multimedia proje...\n3       wine rack concept housewar wr 44526 solid wood...\n4       light bulb wintergreen light christma led ligh...\n                              ...                        \n8120    acoust guitar clamp hot metal acoust electr gu...\n8121    rain jacket urban republ boy  hood stripe rain...\n8122    exten hardisk 500 gb 500gb extern hard drive u...\n8123                 bike lock mighti amsterdam bike lock\n8124    phillip coffe maker hamilton beach flexbrew wa...\nLength: 7558, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:00.815390Z","iopub.execute_input":"2023-11-29T21:02:00.815835Z","iopub.status.idle":"2023-11-29T21:02:00.837692Z","shell.execute_reply.started":"2023-11-29T21:02:00.815792Z","shell.execute_reply":"2023-11-29T21:02:00.836036Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(0.781467760430449, 1.0, 0.0)"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\ntest_x_tfidf = tfidf.transform(test_input)\n\nsvd = TruncatedSVD(n_components=100)\ntrain_x_tfidf = svd.fit_transform(train_x_tfidf)\ndev_x_tfidf = svd.transform(dev_x_tfidf)\ntest_x_tfidf = svd.transform(test_x_tfidf)\n\nprint(train_x_tfidf)\nprint('#############################')\n\n# Add new features\ndistances = []\nquasi_jaccard = []\n\nfor row in train_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ntrain_x = np.column_stack([train_x_tfidf, np.array([distances, quasi_jaccard]).T])\n\nprint(train_x)\n\ndistances = []\nquasi_jaccard = []\n\nfor row in dev_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ndev_x = np.column_stack([dev_x_tfidf, np.array([distances, quasi_jaccard]).T])\n\ndistances = []\nquasi_jaccard = []\n\nfor row in test_x_tfidf:\n    cos_distance = linear_kernel(row[:len(row)//2].reshape(1, -1), row[len(row)//2:].reshape(1, -1))[0][0]\n    intersect = np.dot(row[:len(row)//2], row[len(row)//2:])\n    union = np.sum(row[:len(row)//2]) + np.sum(row[len(row)//2:])\n    quasi_jaccard.append(1.0 * intersect / union)\n    distances.append(cos_distance)\n\ntest_x = np.column_stack([test_x_tfidf, np.array([distances, quasi_jaccard]).T])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:00.839350Z","iopub.execute_input":"2023-11-29T21:02:00.839757Z","iopub.status.idle":"2023-11-29T21:02:10.054825Z","shell.execute_reply.started":"2023-11-29T21:02:00.839720Z","shell.execute_reply":"2023-11-29T21:02:10.053583Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[[ 1.86065134e-02  5.49931053e-02  8.29976840e-03 ... -8.49509537e-02\n   6.71328643e-03  5.12951063e-02]\n [ 3.92515624e-03  3.52257307e-03  8.00647957e-03 ...  1.66663600e-02\n  -3.42247996e-02 -1.36737542e-02]\n [ 2.01549981e-04  1.76536579e-04  4.17149617e-04 ... -3.77253300e-03\n  -1.84218929e-02 -1.63866753e-02]\n ...\n [ 2.89562259e-03  6.74143435e-03  4.77551900e-03 ... -1.70832436e-02\n  -5.07095246e-03  9.57087015e-04]\n [ 1.48038429e-03  1.06522609e-03  3.99882794e-03 ... -4.60429725e-03\n  -8.87996104e-03  2.04921069e-02]\n [ 8.98487477e-03  8.95677848e-03  2.94945426e-01 ...  1.76345093e-02\n  -1.20745965e-02 -3.02919971e-02]]\n#############################\n[[ 1.86065134e-02  5.49931053e-02  8.29976840e-03 ...  5.12951063e-02\n   7.56678709e-05  3.51260927e-04]\n [ 3.92515624e-03  3.52257307e-03  8.00647957e-03 ... -1.36737542e-02\n  -4.12453386e-04 -1.51074159e-03]\n [ 2.01549981e-04  1.76536579e-04  4.17149617e-04 ... -1.63866753e-02\n   2.33852950e-05 -5.43799284e-04]\n ...\n [ 2.89562259e-03  6.74143435e-03  4.77551900e-03 ...  9.57087015e-04\n  -1.93954953e-03 -1.57860771e-01]\n [ 1.48038429e-03  1.06522609e-03  3.99882794e-03 ...  2.04921069e-02\n   7.00225861e-03 -4.30495720e-02]\n [ 8.98487477e-03  8.95677848e-03  2.94945426e-01 ... -3.02919971e-02\n   1.72480679e-03  1.58187796e-02]]\n","output_type":"stream"}]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n# Assuming train_input and dev_input are your raw text data\nfeature_inserter = FeatureInserter()\ntrain_x_with_features = feature_inserter.fit_transform(train_input, train_y)\ndev_x_with_features = feature_inserter.transform(dev_input)\n\n# Assuming train_x and dev_x are your vectorized data\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Concatenate the new features with the TF-IDF vectorized data\ntrain = np.column_stack([train_x_tfidf.toarray(), train_x_with_features])\ndev = np.column_stack([dev_x_tfidf.toarray(), dev_x_with_features])'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:10.056576Z","iopub.execute_input":"2023-11-29T21:02:10.056902Z","iopub.status.idle":"2023-11-29T21:02:10.066193Z","shell.execute_reply.started":"2023-11-29T21:02:10.056872Z","shell.execute_reply":"2023-11-29T21:02:10.064498Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"\"import pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline, FeatureUnion\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import KFold\\nfrom scipy import sparse\\nfrom sklearn.metrics.pairwise import linear_kernel\\nfrom bs4 import BeautifulSoup\\nimport re\\nfrom nltk.stem.porter import PorterStemmer\\nimport string\\nfrom sklearn.feature_extraction import text\\n\\n# ... (previous imports and code)\\n\\nclass FeatureInserter():\\n    def __init__(self):\\n        pass\\n\\n    def transform(self, X, y=None):\\n        distances = []\\n        quasi_jaccard = []\\n\\n        for row in X.tocsr():\\n            row = row.toarray().ravel()\\n\\n            if len(row.shape) == 1:\\n                row = row.reshape(1, -1)\\n\\n            # Split the row into two parts\\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\\n\\n            # Ensure both matrices have the same number of features\\n            min_features = min(part1.shape[1], part2.shape[1])\\n\\n            # Check for zero denominator to avoid division by zero\\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\\n            if denominator == 0:\\n                cos_distance = 0  # or any other suitable value\\n            else:\\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\\n\\n            # Compute quasi-Jaccard similarity\\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\\n            quasi_jaccard.append(1.0 * intersect / union)\\n\\n            distances.append(cos_distance[0])\\n\\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\\n\\n    def fit(self, X, y):\\n        return self\\n\\n    def fit_transform(self, X, y, **fit_params):\\n        self.fit(X, y)\\n        return self.transform(X)\\n\\n# Assuming train_input and dev_input are your raw text data\\nfeature_inserter = FeatureInserter()\\ntrain_x_with_features = feature_inserter.fit_transform(train_input, train_y)\\ndev_x_with_features = feature_inserter.transform(dev_input)\\n\\n# Assuming train_x and dev_x are your vectorized data\\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\\ntrain_x_tfidf = tfidf.fit_transform(train_input)\\ndev_x_tfidf = tfidf.transform(dev_input)\\n\\n# Concatenate the new features with the TF-IDF vectorized data\\ntrain = np.column_stack([train_x_tfidf.toarray(), train_x_with_features])\\ndev = np.column_stack([dev_x_tfidf.toarray(), dev_x_with_features])\""},"metadata":{}}]},{"cell_type":"code","source":"'''import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\n#from sklearn.cross_validation import KFold\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport string\nfrom sklearn.feature_extraction import text\n\n# ... (previous imports and code)\n\nclass FeatureInserter():\n    def __init__(self):\n        pass\n\n    def transform(self, X, y=None):\n        distances = []\n        quasi_jaccard = []\n\n        for row in X.tocsr():\n            row = row.toarray().ravel()\n\n            if len(row.shape) == 1:\n                row = row.reshape(1, -1)\n\n            # Split the row into two parts\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\n\n            # Ensure both matrices have the same number of features\n            min_features = min(part1.shape[1], part2.shape[1])\n\n            # Check for zero denominator to avoid division by zero\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\n            if denominator == 0:\n                cos_distance = 0  # or any other suitable value\n            else:\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\n\n            # Compute quasi-Jaccard similarity\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\n            quasi_jaccard.append(1.0 * intersect / union)\n\n            distances.append(cos_distance[0])\n\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\n\n    def fit(self, X, y):\n        return self\n\n    def fit_transform(self, X, y, **fit_params):\n        self.fit(X, y)\n        return self.transform(X)\n\n\n# Example usage:\nfeature_inserter = FeatureInserter()\n\n# Apply TF-IDF vectorization\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\ntrain_x_tfidf = tfidf.fit_transform(train_input)\ndev_x_tfidf = tfidf.transform(dev_input)\n\n# Apply FeatureInserter\ntrain_x = feature_inserter.fit_transform(train_x_tfidf, train_y)\ndev_x = feature_inserter.transform(dev_x_tfidf)\n\n'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:10.067805Z","iopub.execute_input":"2023-11-29T21:02:10.068233Z","iopub.status.idle":"2023-11-29T21:02:10.089357Z","shell.execute_reply.started":"2023-11-29T21:02:10.068197Z","shell.execute_reply":"2023-11-29T21:02:10.087628Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"\"import pandas as pd\\nimport numpy as np\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline, FeatureUnion\\nfrom sklearn.linear_model import LogisticRegression\\n#from sklearn.cross_validation import KFold\\nfrom scipy import sparse\\nfrom sklearn.metrics.pairwise import linear_kernel\\nfrom bs4 import BeautifulSoup\\nimport re\\nfrom nltk.stem.porter import PorterStemmer\\nimport string\\nfrom sklearn.feature_extraction import text\\n\\n# ... (previous imports and code)\\n\\nclass FeatureInserter():\\n    def __init__(self):\\n        pass\\n\\n    def transform(self, X, y=None):\\n        distances = []\\n        quasi_jaccard = []\\n\\n        for row in X.tocsr():\\n            row = row.toarray().ravel()\\n\\n            if len(row.shape) == 1:\\n                row = row.reshape(1, -1)\\n\\n            # Split the row into two parts\\n            part1, part2 = row[:, :row.shape[1]//2], row[:, row.shape[1]//2:]\\n\\n            # Ensure both matrices have the same number of features\\n            min_features = min(part1.shape[1], part2.shape[1])\\n\\n            # Check for zero denominator to avoid division by zero\\n            denominator = np.linalg.norm(part1[:, :min_features]) * np.linalg.norm(part2[:, :min_features])\\n            if denominator == 0:\\n                cos_distance = 0  # or any other suitable value\\n            else:\\n                cos_distance = 1.0 - np.dot(part1[:, :min_features], part2[:, :min_features].T) / denominator\\n\\n            # Compute quasi-Jaccard similarity\\n            intersect = np.sum(np.minimum(part1[:, :min_features], part2[:, :min_features]))\\n            union = np.sum(np.maximum(part1[:, :min_features], part2[:, :min_features]))\\n            quasi_jaccard.append(1.0 * intersect / union)\\n\\n            distances.append(cos_distance[0])\\n\\n        return np.column_stack([X.toarray(), np.array([distances, quasi_jaccard]).T])\\n\\n    def fit(self, X, y):\\n        return self\\n\\n    def fit_transform(self, X, y, **fit_params):\\n        self.fit(X, y)\\n        return self.transform(X)\\n\\n\\n# Example usage:\\nfeature_inserter = FeatureInserter()\\n\\n# Apply TF-IDF vectorization\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\ntfidf = TfidfVectorizer(ngram_range=(1, 5), stop_words='english', strip_accents='unicode')\\ntrain_x_tfidf = tfidf.fit_transform(train_input)\\ndev_x_tfidf = tfidf.transform(dev_input)\\n\\n# Apply FeatureInserter\\ntrain_x = feature_inserter.fit_transform(train_x_tfidf, train_y)\\ndev_x = feature_inserter.transform(dev_x_tfidf)\\n\\n\""},"metadata":{}}]},{"cell_type":"code","source":"'''train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\ntrain_y = [(x-1)/3 for x in train_y]\ndev_y = [(x-1)/3 for x in dev_y]\nnp.mean(train_y), np.max(train_y), np.min(train_y)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:10.091316Z","iopub.execute_input":"2023-11-29T21:02:10.091751Z","iopub.status.idle":"2023-11-29T21:02:10.104014Z","shell.execute_reply.started":"2023-11-29T21:02:10.091710Z","shell.execute_reply":"2023-11-29T21:02:10.102888Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'train_y, dev_y = train.median_relevance.to_list(), dev.median_relevance.to_list()\\ntrain_y = [(x-1)/3 for x in train_y]\\ndev_y = [(x-1)/3 for x in dev_y]\\nnp.mean(train_y), np.max(train_y), np.min(train_y)'"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, cohen_kappa_score, make_scorer\ndef reg_scorer(true, pred):\n    pred = [min(1, max(0,x)) for x in pred]\n    pred = [int(round((x*3)+1)) for x in pred]\n    true = [int(round((x*3)+1)) for x in true]\n    return cohen_kappa_score(true, pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:10.105022Z","iopub.execute_input":"2023-11-29T21:02:10.106126Z","iopub.status.idle":"2023-11-29T21:02:10.115191Z","shell.execute_reply.started":"2023-11-29T21:02:10.106070Z","shell.execute_reply":"2023-11-29T21:02:10.113632Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\n#clf = LinearRegression().fit(train_x, train_y)\n#clf = SGDRegressor(verbose=1,n_iter_no_change=20).fit(train_x, train_y)\nparam_grid = {'C': [10], 'epsilon':[0.1], 'kernel': ('rbf', 'poly','sigmoid')}\nsvr  = SVR()\nscorer = make_scorer(reg_scorer, greater_is_better=True)\nclf = GridSearchCV(svr, param_grid, verbose=True,scoring=scorer, n_jobs=-1)\nclf.fit(train_x, train_y)\nclf.best_estimator_, clf.best_params_, clf.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:02:10.121494Z","iopub.execute_input":"2023-11-29T21:02:10.121872Z","iopub.status.idle":"2023-11-29T21:06:54.185902Z","shell.execute_reply.started":"2023-11-29T21:02:10.121819Z","shell.execute_reply":"2023-11-29T21:06:54.184547Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 48 candidates, totalling 240 fits\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"(SVR(C=10), {'C': 10, 'epsilon': 0.1, 'kernel': 'rbf'}, 0.14087048850933578)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Random Forest Regression\nrandom_forest_regression = RandomForestRegressor()\n\n# Parameter Grid\n'''random_forest_regression_param_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [True, False]\n}'''\nrandom_forest_regression_param_grid = {\n    #'n_estimators': [50, 200],\n    #'max_depth': [None, 10, 30],\n    #'min_samples_split': [2, 5],\n    #'min_samples_leaf': [2, 4],\n    'bootstrap': [True, False]\n}\n\n#RF HERE\n#rf = GridSearchCV(random_forest_regression, random_forest_regression_param_grid, verbose=True,scoring=scorer, n_jobs=-1, cv=2)\n#rf.fit(train_x, train_y)\n#rf.best_estimator_, rf.best_params_, rf.best_score_","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:06:54.187917Z","iopub.execute_input":"2023-11-29T21:06:54.188260Z","iopub.status.idle":"2023-11-29T21:06:54.327321Z","shell.execute_reply.started":"2023-11-29T21:06:54.188229Z","shell.execute_reply":"2023-11-29T21:06:54.325422Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"## 0.26 is the best score till now\n\npreds = clf.best_estimator_.predict(dev_x)\nmean_squared_error(dev_y, preds),  reg_scorer(dev_y, preds)\n\n#preds_rf = rf.best_estimator_.predict(dev_x)\n#mean_squared_error(dev_y, preds_rf),  reg_scorer(dev_y, preds_rf)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:06:54.331811Z","iopub.execute_input":"2023-11-29T21:06:54.332188Z","iopub.status.idle":"2023-11-29T21:06:55.395511Z","shell.execute_reply.started":"2023-11-29T21:06:54.332157Z","shell.execute_reply":"2023-11-29T21:06:55.394412Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"(0.1056255756315762, 0.16471602305640332)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n'''svr_best_estimator = clf.best_estimator_\nrf_best_estimator = rf.best_estimator_\n\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator)]\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\n\nstacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n\nstacking_regressor.fit(train_x, train_y)\npreds_stacking_regressor = stacking_regressor.predict(dev_x)\nmean_squared_error(dev_y, preds_stacking_regressor),  reg_scorer(dev_y, preds_stacking_regressor)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:06:55.397199Z","iopub.execute_input":"2023-11-29T21:06:55.397596Z","iopub.status.idle":"2023-11-29T21:06:55.405707Z","shell.execute_reply.started":"2023-11-29T21:06:55.397561Z","shell.execute_reply":"2023-11-29T21:06:55.404195Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"\"svr_best_estimator = clf.best_estimator_\\nrf_best_estimator = rf.best_estimator_\\n\\nbase_models = [('svr', svr_best_estimator), ('rf', rf_best_estimator)]\\nmeta_model = LinearRegression()  # You can choose a different meta-model if needed\\n\\nstacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_model)\\n\\nstacking_regressor.fit(train_x, train_y)\\npreds_stacking_regressor = stacking_regressor.predict(dev_x)\\nmean_squared_error(dev_y, preds_stacking_regressor),  reg_scorer(dev_y, preds_stacking_regressor)\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\n#test_x = tfidf.transform(test_input)\n'''pred = stacking_regressor.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:06:55.407848Z","iopub.execute_input":"2023-11-29T21:06:55.408354Z","iopub.status.idle":"2023-11-29T21:06:55.423999Z","shell.execute_reply.started":"2023-11-29T21:06:55.408306Z","shell.execute_reply":"2023-11-29T21:06:55.422948Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"'pred = stacking_regressor.predict(test_x)\\npred = [min(1, max(0,x)) for x in pred]\\npred = [int(round((x*3)+1)) for x in pred]\\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\\nout.to_csv(\\'submission.csv\\', index=False)'"},"metadata":{}}]},{"cell_type":"code","source":"#test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\n#test_x = tfidf.transform(test_input)\npred = clf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:06:55.425945Z","iopub.execute_input":"2023-11-29T21:06:55.426391Z","iopub.status.idle":"2023-11-29T21:07:06.991751Z","shell.execute_reply.started":"2023-11-29T21:06:55.426349Z","shell.execute_reply":"2023-11-29T21:07:06.990952Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"'''test_input =  test.apply(lambda x: x['query']+' '+x['product_title'], axis=1)\ntest_x = tfidf.transform(test_input)\npred = rf.best_estimator_.predict(test_x)\npred = [min(1, max(0,x)) for x in pred]\npred = [int(round((x*3)+1)) for x in pred]\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\nout.to_csv('submission.csv', index=False)'''","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:07:06.992926Z","iopub.execute_input":"2023-11-29T21:07:06.993680Z","iopub.status.idle":"2023-11-29T21:07:07.000610Z","shell.execute_reply.started":"2023-11-29T21:07:06.993651Z","shell.execute_reply":"2023-11-29T21:07:06.999217Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'test_input =  test.apply(lambda x: x[\\'query\\']+\\' \\'+x[\\'product_title\\'], axis=1)\\ntest_x = tfidf.transform(test_input)\\npred = rf.best_estimator_.predict(test_x)\\npred = [min(1, max(0,x)) for x in pred]\\npred = [int(round((x*3)+1)) for x in pred]\\nout = pd.DataFrame({\"id\": test.id.to_list(), \"prediction\": pred})\\nout.to_csv(\\'submission.csv\\', index=False)'"},"metadata":{}}]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/crowdflower-search-relevance/sampleSubmission.csv.zip')\nsub","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:07:07.001885Z","iopub.execute_input":"2023-11-29T21:07:07.002164Z","iopub.status.idle":"2023-11-29T21:07:07.035309Z","shell.execute_reply.started":"2023-11-29T21:07:07.002136Z","shell.execute_reply":"2023-11-29T21:07:07.034306Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"          id  prediction\n0          3           3\n1          6           3\n2          9           3\n3         11           3\n4         12           3\n...      ...         ...\n22508  32665           3\n22509  32667           3\n22510  32669           3\n22511  32670           3\n22512  32671           3\n\n[22513 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22508</th>\n      <td>32665</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22509</th>\n      <td>32667</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22510</th>\n      <td>32669</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22511</th>\n      <td>32670</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22512</th>\n      <td>32671</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>22513 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"out","metadata":{"execution":{"iopub.status.busy":"2023-11-29T21:07:07.036600Z","iopub.execute_input":"2023-11-29T21:07:07.037585Z","iopub.status.idle":"2023-11-29T21:07:07.047228Z","shell.execute_reply.started":"2023-11-29T21:07:07.037551Z","shell.execute_reply":"2023-11-29T21:07:07.046335Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"          id  prediction\n0          3           4\n1          6           4\n2          9           3\n3         11           4\n4         12           4\n...      ...         ...\n22508  32665           4\n22509  32667           4\n22510  32669           3\n22511  32670           4\n22512  32671           4\n\n[22513 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>12</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22508</th>\n      <td>32665</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>22509</th>\n      <td>32667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>22510</th>\n      <td>32669</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>22511</th>\n      <td>32670</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>22512</th>\n      <td>32671</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>22513 rows  2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}